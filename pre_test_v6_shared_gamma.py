"""
Global Fitting v6: Shared Gamma Model
å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…±é€šã®7-gammaãƒ¢ãƒ‡ãƒ«

ã€ç‰©ç†çš„æ ¹æ‹ ã€‘
- Î³â‚–: æº–ä½|kâŸ©ã®å›ºæœ‰ç·©å’Œç‡ï¼ˆææ–™ç‰¹æ€§ã€æ¸©åº¦ãƒ»ç£å ´ã«ä¾å­˜ã—ãªã„ï¼‰
- æ¸©åº¦ä¾å­˜: Boltzmannåˆ†å¸ƒã§è‡ªå‹•çš„ã«è¡¨ç¾
- ç£å ´ä¾å­˜: Zeemanåˆ†è£‚ã§è‡ªå‹•çš„ã«è¡¨ç¾

ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€‘
- v6: 12å€‹ (5 global + 7 shared gamma) â† 84%å‰Šæ¸›

ã€æœŸå¾…åŠ¹æœã€‘
- æ¡ä»¶æ•°æ”¹å–„: 10Â¹â¶ â†’ 10â¶-10â¸
- MCMCåæŸæ€§: R-hat < 1.05, ESS > 400
- ç‰©ç†çš„è§£é‡ˆ: æ˜ç¢ºï¼ˆææ–™å›ºæœ‰å€¤ï¼‰
"""

import os
import pathlib
os.environ['OMP_NUM_THREADS'] = '8'
os.environ['MKL_NUM_THREADS'] = '8'
os.environ['OPENBLAS_NUM_THREADS'] = '8'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import least_squares
from scipy.signal import find_peaks, peak_widths
import unified_weighted_bayesian_fitting_final as uwbf
import warnings
from datetime import datetime
import json
from pathlib import Path
import traceback

warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.dpi'] = 120

# ==========================================
# âš™ï¸ è§£æãƒ¢ãƒ‡ãƒ«è¨­å®š
# ==========================================
MODEL_FORMS = ['B', 'H']

# ç‰©ç†å®šæ•°ï¼ˆè¨ºæ–­ç”¨ï¼‰
kB = 1.380649e-23  # Boltzmannå®šæ•° [J/K]

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆæ¡ä»¶æ•°æœ€é©åŒ–ç‰ˆ v5ï¼‰
# ç›®æ¨™: æœ€é©åŒ–ç©ºé–“ã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¹…ã‚’50ç¨‹åº¦ã«çµ±ä¸€
# å¢ƒç•Œæ‹¡å¼µç‰ˆ: a=[0.1, 5.0], Bâ‚„=[0.1mK, 30mK], Bâ‚†=[-1mK, 1mK]
SCALING_FACTORS = {
    'g': 38.0,      # [1.5, 2.8] â†’ [57, 106] (å¹…49)
    'a': 10.2,      # [0.1, 5.0] â†’ [1.02, 51.0] (å¹…50) - æ‹¡å¼µ
    'B4': 1672.0,   # [1e-4, 3e-2] â†’ [0.17, 50.16] (å¹…50) - æ‹¡å¼µ
    'B6': 25000.0,  # [-1e-3, 1e-3] â†’ [-25, 25] (å¹…50) - æ‹¡å¼µ
    'eps': 17.0,    # [13.0, 16.0] â†’ [221, 272] (å¹…51)
    'gamma': 100.0  # [0.01, 0.5] â†’ [1.0, 50.0] (å¹…49)
}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹æˆ
TARGET_DATA = [
    {'B': 9.0, 'T': 4.0,  'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '4K'},
    {'B': 9.0, 'T': 10.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '10K'},
    {'B': 9.0, 'T': 20.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '20K'},
    {'B': 9.0, 'T': 30.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '30K'},
    {'B': 4.2, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '4.2T'},
    {'B': 5.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '5T'},
    {'B': 6.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '6T'},
    {'B': 7.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '7T'},
    {'B': 8.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '8T'},
    {'B': 9.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '9T'},
]

# ==========================================
# ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†
# ==========================================
def detect_polariton_modes(freq, trans, polariton_upper_limit=0.361505):
    """
    é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰ã‚’æ¤œå‡º
    
    Parameters:
    -----------
    freq : array
        å‘¨æ³¢æ•°é…åˆ— [THz]
    trans : array
        é€éç‡é…åˆ—
    polariton_upper_limit : float
        ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰é ˜åŸŸã®å‘¨æ³¢æ•°ä¸Šé™ [THz] = 0.361505
    
    Returns:
    --------
    has_polariton : bool
        ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰(UP/LP)ãŒæ¤œå‡ºã•ã‚ŒãŸã‹
    """
    from scipy.signal import find_peaks
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆé€éç‡ãŒé«˜ã„éƒ¨åˆ†ï¼‰
    peaks, _ = find_peaks(trans, prominence=0.05, width=3)
    
    if len(peaks) == 0:
        return False
    
    peak_freqs = freq[peaks]
    
    # ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰é ˜åŸŸã®ãƒ”ãƒ¼ã‚¯ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    polariton_peaks = peak_freqs[peak_freqs <= polariton_upper_limit]
    
    # 2å€‹ä»¥ä¸Šã®ä½å‘¨æ³¢ãƒ”ãƒ¼ã‚¯ = ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰å½¢æˆï¼ˆUP/LPï¼‰
    return len(polariton_peaks) >= 2

def detect_peaks_and_classify(freq, trans, polariton_upper_limit=0.361505, cavity_lower_limit=0.45):
    """
    ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡ºã—ã€ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰ vs å…±æŒ¯å™¨ãƒ¢ãƒ¼ãƒ‰ã«åˆ†é¡
    é€éç‡ãŒé«˜ã„éƒ¨åˆ†ï¼ˆé€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ”ãƒ¼ã‚¯ï¼‰ã‚’æ¤œå‡º
    """
    from scipy.signal import find_peaks
    
    # é€éç‡ã®æ¥µå¤§å€¤ã‚’æ¤œå‡º
    peaks, properties = find_peaks(trans, prominence=0.05, width=3)
    
    if len(peaks) == 0:
        return [], []
    
    peak_freqs = freq[peaks]
    peak_widths = properties['widths'] * (freq[1] - freq[0])
    
    sort_idx = np.argsort(peak_freqs)
    peak_freqs = peak_freqs[sort_idx]
    peak_widths = peak_widths[sort_idx]
    
    polariton_regions = []
    cavity_regions = []
    
    for pf, pw in zip(peak_freqs, peak_widths):
        f_start = max(freq[0], pf - 1.5 * pw)
        f_end = min(freq[-1], pf + 1.5 * pw)
        
        if pf <= polariton_upper_limit:
            f_end_clipped = min(f_end, polariton_upper_limit)
            if f_end_clipped > f_start:
                polariton_regions.append((f_start, f_end_clipped))
        elif pf >= cavity_lower_limit:
            f_start_clipped = max(f_start, cavity_lower_limit)
            if f_end > f_start_clipped:
                cavity_regions.append((f_start_clipped, f_end))
    
    return polariton_regions, cavity_regions

def create_weight_array(freq, trans, polariton_regions, cavity_regions):
    """é‡ã¿é…åˆ—ç”Ÿæˆ: ãƒãƒ©ãƒªãƒˆãƒ³=1.5, å…±æŒ¯å™¨=1.0, ãã‚Œä»¥å¤–=0.01"""
    weight_array = np.full_like(freq, 0.01)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01
    
    # ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸ: 1.5
    for f_start, f_end in polariton_regions:
        mask = (freq >= f_start) & (freq <= f_end)
        weight_array[mask] = 1.5
    
    # å…±æŒ¯å™¨é ˜åŸŸ: 1.0
    for f_start, f_end in cavity_regions:
        mask = (freq >= f_start) & (freq <= f_end)
        weight_array[mask] = 1.0
    
    return weight_array

def load_all_datasets(target_data_list):
    """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---")
    
    datasets = []
    base_dir = Path(__file__).parent / 'bayesian_inputs'
    
    for idx, config in enumerate(target_data_list):
        excel_path = base_dir / config['file']
        
        if not excel_path.exists():
            print(f"âŒ {excel_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue
        
        try:
            # Excelã‚·ãƒ¼ãƒˆã¨åˆ—åã‹ã‚‰èª­ã¿è¾¼ã¿
            df = pd.read_excel(excel_path, sheet_name=config['sheet'])
            
            # å‘¨æ³¢æ•°åˆ—ï¼ˆå…±é€šï¼‰
            if 'Frequency (THz)' not in df.columns:
                print(f"âŒ {config['col']}: å‘¨æ³¢æ•°åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            # ãƒ‡ãƒ¼ã‚¿åˆ—ï¼ˆå„æ¡ä»¶ï¼‰
            if config['col'] not in df.columns:
                print(f"âŒ {config['col']}: ãƒ‡ãƒ¼ã‚¿åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            df_clean = df[['Frequency (THz)', config['col']]].dropna()
            freq = df_clean['Frequency (THz)'].values.astype(np.float64)
            trans = df_clean[config['col']].values.astype(np.float64)
            
            # å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ç”Ÿæˆ
            polariton_regions, cavity_regions = detect_peaks_and_classify(freq, trans)
            weight_array = create_weight_array(freq, trans, polariton_regions, cavity_regions)
            
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆB/Tæ¡ä»¶ï¼‰
            if config['T'] == 1.5:
                label = f"{config['B']:.1f}T"
            else:
                label = f"{config['T']:.0f}K"
            
            dataset = {
                'freq': freq,
                'trans': trans,
                'weight_array': weight_array,  # å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—
                'B': config['B'],
                'T': config['T'],
                'label': label,
                'polariton_regions': polariton_regions,
                'cavity_regions': cavity_regions,
                'sigma': 0.01  # åŸºæœ¬ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            }
            
            datasets.append(dataset)
            
            print(f"âœ“ {label} (B={config['B']}T, T={config['T']}K): {len(freq)} points")
            print(f"  Polaritoné ˜åŸŸ (1.5Ã—): {len(polariton_regions)} regions")
            print(f"  Cavityé ˜åŸŸ (1.0Ã—): {len(cavity_regions)} regions")
            
        except Exception as e:
            print(f"âŒ {config['col']} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if len(datasets) == 0:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ1ã¤ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print(f"\nâœ… åˆè¨ˆ {len(datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
    
    return datasets

def pack_shared_gamma_parameters(global_dict, gamma_shared):
    """
    å…±æœ‰gammaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’1æ¬¡å…ƒé…åˆ—ã«å¤‰æ›
    
    Parameters
    ----------
    global_dict : dict
        {'g', 'a', 'B4', 'B6', 'eps'}
    gamma_shared : np.ndarray
        [7å€‹] å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å…±é€š
    
    Returns
    -------
    params_flat : np.ndarray
        [g, a, B4, B6, eps, Î³â‚, Î³â‚‚, ..., Î³â‚‡]
        åˆè¨ˆï¼š5 + 7 = 12å€‹
    """
    params_flat = [
        global_dict['g'] * SCALING_FACTORS['g'],
        global_dict['a'] * SCALING_FACTORS['a'],
        global_dict['B4'] * SCALING_FACTORS['B4'],
        global_dict['B6'] * SCALING_FACTORS['B6'],
        global_dict['eps'] * SCALING_FACTORS['eps']
    ]
    
    for g in gamma_shared:
        params_flat.append(g * SCALING_FACTORS['gamma'])
    
    return np.array(params_flat)

def unpack_shared_gamma_parameters(params_flat):
    """1æ¬¡å…ƒé…åˆ—ã‚’è¾æ›¸ã«åˆ†è§£"""
    global_scaled = {
        'g': params_flat[0],
        'a': params_flat[1],
        'B4': params_flat[2],
        'B6': params_flat[3],
        'eps': params_flat[4]
    }
    
    gamma_shared = params_flat[5:12]  # 7å€‹
    
    return global_scaled, gamma_shared

# ==========================================
# ğŸ¯ å…±æœ‰Gamma Residuals
# ==========================================
def shared_gamma_residuals(params_flat, datasets, model_form='H'):
    """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åŒã˜gammaã‚’ä½¿ç”¨ã—ãŸæ®‹å·®è¨ˆç®—"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å˜ä½ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    
    # ç‰©ç†çš„ç¯„å›²ã«åˆ¶é™
    gamma_array = np.clip(gamma_array, 0.005, 0.4)
    
    residuals = []
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    for data in datasets:
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®B, Tï¼‰
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        
        # æ„Ÿå—ç‡ï¼ˆå…±é€šã®gammaä½¿ç”¨ï¼‰
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        
        # æ¯”é€ç£ç‡
        if model_form == 'H':
            mu_r = 1.0 + chi
        elif model_form == 'B':
            denominator = 1.0 - chi
            mu_r = 1.0 / denominator
        
        # é€éç‡
        trans = uwbf.calculate_transmission(data['freq'], mu_r, d_fixed, eps_bg)
        
        # NaNãƒã‚§ãƒƒã‚¯
        if np.any(~np.isfinite(trans)):
            trans = np.nan_to_num(trans, nan=0.5)
        
        # å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿ä»˜ãæ®‹å·®
        # weight_array: ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸ=1.5, ãã‚Œä»¥å¤–=1.0
        effective_sigma = data['sigma'] / np.sqrt(data['weight_array'])
        res = (data['trans'] - trans) / effective_sigma
        residuals.append(res)
    
    return np.concatenate(residuals)

# ==========================================
# ğŸš€ åˆæœŸå€¤ãƒ»å¢ƒç•Œå€¤
# ==========================================
def generate_shared_gamma_initial_values():
    """å…±æœ‰gammaãƒ¢ãƒ‡ãƒ«ã®åˆæœŸå€¤"""
    print("\nğŸ”§ Generating shared gamma initial values...")
    
    # Global parameters
    global_phys = {
        'g': 1.95,
        'a': 1.0,      
        'B4': 2.02*1.0e-3, # å±±ç”°ã®è«–æ–‡å€¤å‚è€ƒ
        'B6': -1.2*1.0e-5, # å±±ç”°ã®è«–æ–‡å€¤å‚è€ƒ 
        'eps': 14.4        # Elijahã‚‰ã®å®Ÿé¨“å€¤å‚è€ƒ
    }
    
    # å…±æœ‰gammaï¼ˆææ–™å›ºæœ‰å€¤ã®æ¨å®šï¼‰
    # pre_test_v4ã®çµæœã‹ã‚‰å…¸å‹çš„ãªå€¤ã‚’æŠ½å‡º
    gamma_shared = np.array([0.10, 0.15, 0.12, 0.11, 0.14, 0.13, 0.16])
    
    return global_phys, gamma_shared

def get_shared_gamma_bounds():
    """å…±æœ‰gammaãƒ¢ãƒ‡ãƒ«ã®å¢ƒç•Œå€¤
    
    ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã®ç‰©ç†çš„æ ¹æ‹ ã€‘
    
    Bâ‚„, Bâ‚† (çµæ™¶å ´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿):
        - GdÂ³âºã‚¤ã‚ªãƒ³ (4fâ·, S=7/2) ã®å¸ŒåœŸé¡ã‚¬ãƒ¼ãƒãƒƒãƒˆçµæ™¶ã§ä¸€èˆ¬çš„ãªç¯„å›²ã‚’æ¡ç”¨
        - Bâ‚„: 0.5 mK ï½ 20 mK (å…¸å‹å€¤: 1-5 mK)
        - Bâ‚†: Â±0.5 mK (Bâ‚„ã‚ˆã‚Š1-2æ¡å°ã•ã„)
        - æ³¨: å±±ç”°ã®è«–æ–‡å€¤ (Bâ‚„=2.02mK, Bâ‚†=-1.2Ã—10â»âµK) ã¯å‚è€ƒå€¤ã¨ã—ã¦ä½¿ç”¨
    
    a_scale (çµåˆå®šæ•°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°):
        - Gâ‚€ = a Ã— (Î¼â‚€ N_spin (g Î¼_B)Â²) / (2â„) / THz_TO_RAD_S
        - ç†è«–çš„ã«ã¯ a = 1.0 ã ãŒã€ä»¥ä¸‹ã®ä¸ç¢ºå®šæ€§ã‚’è€ƒæ…®:
          * ã‚µãƒ³ãƒ—ãƒ«åšã•ã®èª¤å·® (Â±20-50%)
          * ã‚¹ãƒ”ãƒ³å¯†åº¦ N_spin ã®ä¸ç¢ºå®šæ€§
          * å…‰å­¦å®šæ•°ã®è£œæ­£
        - æ‹¡å¼µç¯„å›²: 0.1 ï½ 5.0 (å®Ÿé¨“èª¤å·®Â±5å€ã‚’è¨±å®¹)
    """
    # Global parameters
    g_min, g_max = 1.5, 2.8           # gå› å­: GdÂ³âºã®ä¸€èˆ¬å€¤ ï½2.0 (ç¶­æŒ)
    a_min, a_max = 0.1, 5.0           # æ‹¡å¼µ: [0.3, 5.0] â†’ [0.1, 5.0] å®Ÿé¨“èª¤å·®å¯¾å¿œ
    B4_min, B4_max = 1.0e-4, 3.0e-2   # æ‹¡å¼µ: GdÂ³âºã‚¬ãƒ¼ãƒãƒƒãƒˆ (0.1-30 mK) H/Bå½¢å¼å¯¾å¿œ
    B6_min, B6_max = -1.0e-3, 1.0e-3  # æ‹¡å¼µ: [-0.5mK, 0.5mK] â†’ [-1mK, 1mK]
    eps_min, eps_max = 13.0, 16.0     # èª˜é›»ç‡: GGGä¸€èˆ¬å€¤ (ç¶­æŒ)
    
    # Shared gamma (ç¶­æŒ: ç¾åœ¨ã®è¨­å®šã§ç‰©ç†çš„å¦¥å½“æ€§ã‚ã‚Š)
    gamma_min, gamma_max = 0.01, 0.5
    
    lower = [
        g_min * SCALING_FACTORS['g'],
        a_min * SCALING_FACTORS['a'],
        B4_min * SCALING_FACTORS['B4'],
        B6_min * SCALING_FACTORS['B6'],
        eps_min * SCALING_FACTORS['eps']
    ]
    upper = [
        g_max * SCALING_FACTORS['g'],
        a_max * SCALING_FACTORS['a'],
        B4_max * SCALING_FACTORS['B4'],
        B6_max * SCALING_FACTORS['B6'],
        eps_max * SCALING_FACTORS['eps']
    ]
    
    # 7å€‹ã®å…±æœ‰gamma
    lower.extend([gamma_min * SCALING_FACTORS['gamma']] * 7)
    upper.extend([gamma_max * SCALING_FACTORS['gamma']] * 7)
    
    return np.array(lower), np.array(upper)

# ==========================================
# ğŸ“Š ãƒ•ã‚£ãƒƒãƒˆå“è³ªè§£æ
# ==========================================
def analyze_shared_gamma_fit_quality(datasets, params_flat, model_form):
    """ãƒ•ã‚£ãƒƒãƒˆå“è³ªã®çµ±è¨ˆåˆ†æ"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    gamma_array = np.clip(gamma_array, 0.01, 0.5)
    
    stats = []
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    for data in datasets:
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        
        if model_form == 'H':
            mu_r = 1.0 + chi
        elif model_form == 'B':
            denominator = 1.0 - chi
            mu_r = 1.0 / denominator
        
        trans = uwbf.calculate_transmission(data['freq'], mu_r, d_fixed, eps_bg)
        trans = np.nan_to_num(trans, nan=0.5)
        
        residuals = data['trans'] - trans
        rmse = np.sqrt(np.mean(residuals**2))
        max_error = np.max(np.abs(residuals))
        r_squared = 1 - np.sum(residuals**2) / np.sum((data['trans'] - np.mean(data['trans']))**2)
        
        chi_abs = np.abs(chi)
        unstable_fraction = 0.0
        if model_form == 'B':
            unstable_mask = chi_abs > 0.9
            unstable_fraction = np.sum(unstable_mask) / len(chi_abs) * 100
        
        stats.append({
            'label': data['label'],
            'B': data['B'],
            'T': data['T'],
            'rmse': rmse,
            'max_error': max_error,
            'r_squared': r_squared,
            'chi_max': np.max(chi_abs),
            'chi_mean': np.mean(chi_abs),
            'unstable_%': unstable_fraction
        })
    
    return pd.DataFrame(stats)

# ==========================================
# ğŸ” Diagnostic Functions
# ==========================================
def diagnose_problematic_regions(datasets, params_flat, model_form):
    """
    å¤±æ•—é ˜åŸŸã®ç‰©ç†çš„è¨ºæ–­
    - Boltzmannåˆ†å¸ƒï¼ˆåŸºåº•çŠ¶æ…‹å æœ‰ç‡ï¼‰
    - ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚®ãƒ£ãƒƒãƒ—
    - Ï‡ã®æœ€å¤§å€¤
    - æ•°å€¤å®‰å®šæ€§ã®è©•ä¾¡
    """
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    gamma_array = np.clip(gamma_array, 0.005, 0.5)
    
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    print("\n" + "="*80)
    print("ğŸ” Physical Diagnosis of Problematic Regions")
    print("="*80)
    
    diagnostics = []
    
    for data in datasets:
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³å›ºæœ‰å€¤è¨ˆç®—
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        E_vals, U = np.linalg.eigh(H)
        
        # Boltzmannåˆ†å¸ƒ
        kT = kB * data['T']  # [J]
        E_vals_J = E_vals * uwbf.hbar * uwbf.THZ_TO_RAD_S  # [J]ã«å¤‰æ›
        Z = np.sum(np.exp(-E_vals_J / kT))
        pops = np.exp(-E_vals_J / kT) / Z
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚®ãƒ£ãƒƒãƒ—ï¼ˆmeVï¼‰
        energy_gap_meV = (E_vals[1] - E_vals[0]) * uwbf.hbar * uwbf.THZ_TO_RAD_S * 1000 / 1.602176634e-19
        
        # æ„Ÿå—ç‡è¨ˆç®—
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        chi_abs = np.abs(chi)
        chi_max = np.max(chi_abs)
        chi_mean = np.mean(chi_abs)
        
        # B-formå®‰å®šæ€§
        if model_form == 'B':
            denominator = 1.0 - chi
        else:
            unstable_fraction = 0.0
        
        # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ª
        if model_form == 'H':
            mu_r = 1.0 + chi
        elif model_form == 'B':
            denominator = 1.0 - chi
            mu_r = 1.0 / denominator
        
        trans_fit = uwbf.calculate_transmission(data['freq'], mu_r, d_fixed, eps_bg)
        trans_fit = np.nan_to_num(trans_fit, nan=0.5)
        
        residuals = data['trans'] - trans_fit
        rmse = np.sqrt(np.mean(residuals**2))
        max_error = np.max(np.abs(residuals))
        r_squared = 1 - np.sum(residuals**2) / np.sum((data['trans'] - np.mean(data['trans']))**2)
        
        chi_abs = np.abs(chi)
        unstable_fraction = 0.0
        if model_form == 'B':
            unstable_mask = chi_abs > 0.9
            unstable_fraction = np.sum(unstable_mask) / len(chi_abs) * 100
        
        # è¨ºæ–­ãƒ•ãƒ©ã‚°
        warnings = []
        if pops[0] > 0.95:
            warnings.append("åŸºåº•çŠ¶æ…‹æ”¯é…çš„(>95%)")
        if chi_max > 1.0 and model_form == 'B':
            warnings.append("B-formæ•°å€¤ä¸å®‰å®š(|Ï‡|>1)")
        if chi_max > 0.8 and model_form == 'H':
            warnings.append("H-form |Ï‡|é«˜(>0.8)")
        if rmse > 0.15:
            warnings.append(f"ãƒ•ã‚£ãƒƒãƒˆå¤±æ•—(RMSE={rmse:.3f})")
        
        status = "âš ï¸ å•é¡Œã‚ã‚Š" if warnings else "âœ… æ­£å¸¸"
        
        diagnostics.append({
            'label': data['label'],
            'B': data['B'],
            'T': data['T'],
            'pop_ground': pops[0],
            'pop_1st_excited': pops[1],
            'energy_gap_meV': energy_gap_meV,
            'chi_max': chi_max,
            'chi_mean': chi_mean,
            'rmse': rmse,
            'status': status,
            'warnings': '; '.join(warnings) if warnings else 'None'
        })
        
        print(f"\n{data['label']} (B={data['B']}T, T={data['T']}K) {status}")
        print(f"  åŸºåº•çŠ¶æ…‹å æœ‰ç‡: {pops[0]:.4f} (ç¬¬1åŠ±èµ·: {pops[1]:.4f})")
        print(f"  ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚®ãƒ£ãƒƒãƒ— (Eâ‚-Eâ‚€): {energy_gap_meV:.3f} meV (vs kT={data['T']*0.0862:.2f} meV)")
        print(f"  Max|Ï‡|: {chi_max:.3f}, Mean|Ï‡|: {chi_mean:.3f}")
        print(f"  RMSE: {rmse:.4f}")
        if warnings:
            print(f"  âš ï¸ è­¦å‘Š: {'; '.join(warnings)}")
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“Š Diagnostic Summary")
    print("="*80)
    
    df_diag = pd.DataFrame(diagnostics)
    
    # å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŠ½å‡º
    problematic = df_diag[df_diag['status'].str.contains('å•é¡Œ')]
    if len(problematic) > 0:
        print(f"\nâš ï¸ å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(problematic)}/{len(df_diag)}")
        print(problematic[['label', 'B', 'T', 'pop_ground', 'chi_max', 'rmse', 'warnings']].to_string(index=False))
        
        # é™¤å¤–æ¨å¥¨ã®åˆ¤å®š
        severe = df_diag[df_diag['rmse'] > 0.15]
        if len(severe) > 0:
            print(f"\nğŸ”´ é™¤å¤–æ¨å¥¨ï¼ˆRMSE > 0.15ï¼‰: {len(severe)}ä»¶")
            for _, row in severe.iterrows():
                print(f"  - {row['label']}: RMSE={row['rmse']:.3f}, |Ï‡|_max={row['chi_max']:.2f}")
    else:
        print("\nâœ… å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ­£å¸¸")
    
    # ç‰©ç†çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
    print("\n" + "-"*80)
    print("ğŸ”¬ Physical Pattern Analysis")
    print("-"*80)
    
    # ä½æ¸©é ˜åŸŸ
    low_temp = df_diag[df_diag['T'] <= 10]
    if len(low_temp) > 0:
        print(f"\nä½æ¸©é ˜åŸŸ (T â‰¤ 10K): {len(low_temp)}ä»¶")
        print(f"  å¹³å‡åŸºåº•å æœ‰ç‡: {low_temp['pop_ground'].mean():.3f}")
        print(f"  å¹³å‡RMSE: {low_temp['rmse'].mean():.4f}")
        print(f"  å¹³å‡|Ï‡|_max: {low_temp['chi_max'].mean():.3f}")
    
    # é«˜ç£å ´é ˜åŸŸ
    high_field = df_diag[df_diag['B'] >= 8.0]
    if len(high_field) > 0:
        print(f"\né«˜ç£å ´é ˜åŸŸ (B â‰¥ 8T): {len(high_field)}ä»¶")
        print(f"  å¹³å‡RMSE: {high_field['rmse'].mean():.4f}")
        print(f"  å¹³å‡|Ï‡|_max: {high_field['chi_max'].mean():.3f}")
    
    # æ¥µç«¯æ¡ä»¶ï¼ˆä½æ¸©+é«˜ç£å ´ï¼‰
    extreme = df_diag[(df_diag['T'] <= 10) & (df_diag['B'] >= 8.0)]
    if len(extreme) > 0:
        print(f"\næ¥µç«¯æ¡ä»¶ (T â‰¤ 10K & B â‰¥ 8T): {len(extreme)}ä»¶")
        print(f"  å¹³å‡RMSE: {extreme['rmse'].mean():.4f}")
        if extreme['rmse'].mean() > 0.15:
            print("  âš ï¸ ã“ã®é ˜åŸŸã¯å˜ã‚¤ã‚ªãƒ³è¿‘ä¼¼ã®é©ç”¨ç¯„å›²å¤–ã®å¯èƒ½æ€§")
    
    print("\n" + "="*80)
    
    return df_diag

# ==========================================
# ğŸ“Š Plotting Functions
# ==========================================
def plot_all_fits(datasets, params_flat, output_dir, model_form):
    """å…¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ•ã‚£ãƒƒãƒˆçµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆé‡ã¿ä»˜ã‘é ˜åŸŸã‚’æ°´è‰²ã§å¼·èª¿ï¼‰"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    gamma_array = np.clip(gamma_array, 0.005, 0.5)
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    for i, (data, ax) in enumerate(zip(datasets, axes)):
        # ãƒ•ã‚£ãƒƒãƒˆã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        
        if model_form == 'H':
            mu_r = 1.0 + chi
        elif model_form == 'B':
            denominator = 1.0 - chi
            mu_r = 1.0 / denominator
        
        y_fit = uwbf.calculate_transmission(data['freq'], mu_r, d_fixed, eps_bg)
        y_fit = np.nan_to_num(y_fit, nan=0.5)
        
        # ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰ã¨å…±æŒ¯å™¨ãƒ¢ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã—ã¦è‰²åˆ†ã‘
        polariton_regions, cavity_regions = detect_peaks_and_classify(data['freq'], y_fit)
        
        # ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸï¼ˆ1.5Ã—é‡ã¿ï¼‰ã‚’èµ¤ã§å¡—ã‚Šã¤ã¶ã—
        polariton_legend_added = False
        for freq_start, freq_end in polariton_regions:
            label = 'Polariton (1.5Ã—)' if not polariton_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.15, color='orange', label=label, zorder=1)
            polariton_legend_added = True
        
        # å…±æŒ¯å™¨é ˜åŸŸï¼ˆ1.0Ã—é‡ã¿ï¼‰ã‚’æ°´è‰²ã§å¡—ã‚Šã¤ã¶ã—
        cavity_legend_added = False
        for freq_start, freq_end in cavity_regions:
            label = 'Cavity (1.0Ã—)' if not cavity_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.15, color='green', label=label, zorder=1)
            cavity_legend_added = True
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚£ãƒƒãƒˆï¼ˆé‡ã¿ä»˜ã‘é ˜åŸŸã®ä¸Šã«æç”»ï¼‰
        ax.plot(data['freq'], data['trans'], 'o', color='gray', 
                markersize=2.5, alpha=0.6, label='Data', zorder=2)
        ax.plot(data['freq'], y_fit, 'r-', linewidth=2.0, label='Fit', zorder=3)
        
        # æ®‹å·®ã®è¡¨ç¤º
        residuals = data['trans'] - y_fit
        rmse = np.sqrt(np.mean(residuals**2))
        
        ax.set_title(f"{data['label']} (RMSE: {rmse:.4f})", fontsize=11, fontweight='bold')
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Transmittance', fontsize=10)
        ax.legend(fontsize=8, loc='best', framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--', linewidth=0.5, zorder=1)
        
        # yè»¸ç¯„å›²ã®è‡ªå‹•èª¿æ•´
        y_margin = 0.05
        y_min = min(np.min(data['trans']), np.min(y_fit)) - y_margin
        y_max = max(np.max(data['trans']), np.max(y_fit)) + y_margin
        ax.set_ylim(y_min, y_max)
    
    # æœªä½¿ç”¨ã®è»¸ã‚’éè¡¨ç¤º
    for j in range(i+1, len(axes)):
        axes[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'fit_all_spectra.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ fit_all_spectra.png saved")

def plot_all_fits_comparison(datasets, params_H, params_B, output_dir):
    """Hå½¢å¼ã¨Bå½¢å¼ã‚’1æšã®ã‚°ãƒ©ãƒ•ã«é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆï¼ˆH:èµ¤, B:é’ï¼‰"""
    # Hå½¢å¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    global_H, gamma_H = unpack_shared_gamma_parameters(params_H)
    g_H = global_H['g'] / SCALING_FACTORS['g']
    a_H = global_H['a'] / SCALING_FACTORS['a']
    B4_H = global_H['B4'] / SCALING_FACTORS['B4']
    B6_H = global_H['B6'] / SCALING_FACTORS['B6']
    eps_H = global_H['eps'] / SCALING_FACTORS['eps']
    gamma_H_array = np.clip(gamma_H / SCALING_FACTORS['gamma'], 0.005, 0.5)
    
    # Bå½¢å¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    global_B, gamma_B = unpack_shared_gamma_parameters(params_B)
    g_B = global_B['g'] / SCALING_FACTORS['g']
    a_B = global_B['a'] / SCALING_FACTORS['a']
    B4_B = global_B['B4'] / SCALING_FACTORS['B4']
    B6_B = global_B['B6'] / SCALING_FACTORS['B6']
    eps_B = global_B['eps'] / SCALING_FACTORS['eps']
    gamma_B_array = np.clip(gamma_B / SCALING_FACTORS['gamma'], 0.005, 0.5)
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    for i, (data, ax) in enumerate(zip(datasets, axes)):
        # Hå½¢å¼ãƒ•ã‚£ãƒƒãƒˆè¨ˆç®—
        H_ham_H = uwbf.get_hamiltonian(data['B'], g_H, B4_H, B6_H)
        chi_raw_H = uwbf.calculate_susceptibility(data['freq'], H_ham_H, data['T'], gamma_H_array)
        G0_H = a_H * uwbf.mu0 * N_spin * (g_H * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi_H = G0_H * chi_raw_H
        mu_r_H = 1.0 + chi_H
        trans_H = uwbf.calculate_transmission(data['freq'], mu_r_H, d_fixed, eps_H)
        trans_H = np.nan_to_num(trans_H, nan=0.5)
        
        # Bå½¢å¼ãƒ•ã‚£ãƒƒãƒˆè¨ˆç®—
        H_ham_B = uwbf.get_hamiltonian(data['B'], g_B, B4_B, B6_B)
        chi_raw_B = uwbf.calculate_susceptibility(data['freq'], H_ham_B, data['T'], gamma_B_array)
        G0_B = a_B * uwbf.mu0 * N_spin * (g_B * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi_B = G0_B * chi_raw_B
        denominator_B = 1.0 - chi_B
        mu_r_B = 1.0 / denominator_B
        trans_B = uwbf.calculate_transmission(data['freq'], mu_r_B, d_fixed, eps_B)
        trans_B = np.nan_to_num(trans_B, nan=0.5)
        
        # ãƒãƒ©ãƒªãƒˆãƒ³/å…±æŒ¯å™¨é ˜åŸŸã®æ¤œå‡ºï¼ˆHå½¢å¼åŸºæº–ï¼‰
        polariton_regions, cavity_regions = detect_peaks_and_classify(data['freq'], trans_H)
        
        # é ˜åŸŸã®å¡—ã‚Šã¤ã¶ã—
        polariton_legend_added = False
        for freq_start, freq_end in polariton_regions:
            label = 'Polariton (1.5Ã—)' if not polariton_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.12, color='orange', label=label, zorder=1)
            polariton_legend_added = True
        
        cavity_legend_added = False
        for freq_start, freq_end in cavity_regions:
            label = 'Cavity (1.0Ã—)' if not cavity_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.12, color='green', label=label, zorder=1)
            cavity_legend_added = True
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚£ãƒƒãƒˆçµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(data['freq'], data['trans'], 'o', color='gray', 
                markersize=2.5, alpha=0.6, label='Data', zorder=2)
        ax.plot(data['freq'], trans_H, '-', color='red', linewidth=2.0, 
                label='H-form', zorder=3)
        ax.plot(data['freq'], trans_B, '-', color='blue', linewidth=2.0, 
                label='B-form', zorder=3)
        
        # RMSEè¨ˆç®—
        rmse_H = np.sqrt(np.mean((data['trans'] - trans_H)**2))
        rmse_B = np.sqrt(np.mean((data['trans'] - trans_B)**2))
        
        ax.set_title(f"{data['label']}\nH-RMSE: {rmse_H:.4f}, B-RMSE: {rmse_B:.4f}", 
                    fontsize=10, fontweight='bold')
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Transmittance', fontsize=10)
        ax.legend(fontsize=7, loc='best', framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--', linewidth=0.5, zorder=1)
        
        # yè»¸ç¯„å›²ã®è‡ªå‹•èª¿æ•´
        y_margin = 0.05
        y_min = min(np.min(data['trans']), np.min(trans_H), np.min(trans_B)) - y_margin
        y_max = max(np.max(data['trans']), np.max(trans_H), np.max(trans_B)) + y_margin
        ax.set_ylim(y_min, y_max)
    
    # æœªä½¿ç”¨ã®è»¸ã‚’éè¡¨ç¤º
    for j in range(i+1, len(axes)):
        axes[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'fit_all_spectra_HB_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ fit_all_spectra_HB_comparison.png saved")

def plot_residuals(datasets, params_flat, output_dir, model_form):
    """æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç³»çµ±èª¤å·®ã®æ¤œå‡ºç”¨ï¼‰"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    gamma_array = np.clip(gamma_array, 0.005, 0.4)
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    N_spin = 1.9386e+28
    d_fixed = 157.8e-6
    
    for i, (data, ax) in enumerate(zip(datasets, axes)):
        # ãƒ•ã‚£ãƒƒãƒˆã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        
        if model_form == 'H':
            mu_r = 1.0 + chi
        elif model_form == 'B':
            denominator = 1.0 - chi
            mu_r = 1.0 / denominator
        
        y_fit = uwbf.calculate_transmission(data['freq'], mu_r, d_fixed, eps_bg)
        y_fit = np.nan_to_num(y_fit, nan=0.5)
        
        residuals = data['trans'] - y_fit
        
        # ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰ã¨å…±æŒ¯å™¨ãƒ¢ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã—ã¦è‰²åˆ†ã‘
        polariton_regions, cavity_regions = detect_peaks_and_classify(data['freq'], y_fit)
        
        # ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸï¼ˆ1.5Ã—é‡ã¿ï¼‰ã‚’ã‚ªãƒ¬ãƒ³ã‚¸ã§å¡—ã‚Šã¤ã¶ã—
        polariton_legend_added = False
        for freq_start, freq_end in polariton_regions:
            label = 'Polariton (1.5Ã—)' if not polariton_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.15, color='orange', label=label, zorder=1)
            polariton_legend_added = True
        
        # å…±æŒ¯å™¨é ˜åŸŸï¼ˆ1.0Ã—é‡ã¿ï¼‰ã‚’ç·‘ã§å¡—ã‚Šã¤ã¶ã—
        cavity_legend_added = False
        for freq_start, freq_end in cavity_regions:
            label = 'Cavity (1.0Ã—)' if not cavity_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.15, color='green', label=label, zorder=1)
            cavity_legend_added = True
        
        # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(data['freq'], residuals, 'o-', color='steelblue', 
                markersize=3, linewidth=1, alpha=0.7, zorder=2)
        ax.axhline(0, color='red', linestyle='--', linewidth=1.5, 
                  label='Zero Line', zorder=1)
        
        # çµ±è¨ˆæƒ…å ±
        rmse = np.sqrt(np.mean(residuals**2))
        mean_res = np.mean(residuals)
        
        ax.set_title(f"{data['label']} (RMSE: {rmse:.4f}, Mean: {mean_res:.4f})", 
                    fontsize=11, fontweight='bold')
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Residuals', fontsize=10)
        ax.legend(fontsize=8, loc='best', framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--', linewidth=0.5, zorder=1)
    
    # æœªä½¿ç”¨ã®è»¸ã‚’éè¡¨ç¤º
    for j in range(i+1, len(axes)):
        axes[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'residuals_all_spectra.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ residuals_all_spectra.png saved")

def plot_chi_distribution(datasets, params_flat, output_dir, model_form):
    """Ï‡åˆ†å¸ƒã®å¯è¦–åŒ–ï¼ˆH-form vs B-formè¨ºæ–­ç”¨ï¼‰"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    gamma_array = gamma_shared / SCALING_FACTORS['gamma']
    gamma_array = np.clip(gamma_array, 0.005, 0.4)
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    N_spin = 1.9386e+28
    
    for i, (data, ax) in enumerate(zip(datasets, axes)):
        # Ï‡è¨ˆç®—
        H = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        chi_raw = uwbf.calculate_susceptibility(data['freq'], H, data['T'], gamma_array)
        G0 = a_scale * uwbf.mu0 * N_spin * (g * uwbf.muB)**2 / (2 * uwbf.hbar) / uwbf.THZ_TO_RAD_S
        chi = G0 * chi_raw
        
        # å®Ÿéƒ¨ã¨è™šéƒ¨ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        chi_real = np.real(chi)
        chi_imag = np.imag(chi)
        
        ax.plot(data['freq'], chi_real, '-', color='blue', linewidth=2.0, label="Re(Ï‡)")
        ax.plot(data['freq'], chi_imag, '--', color='red', linewidth=2.0, label="Im(Ï‡)")
        ax.axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)  # ã‚¼ãƒ­åŸºæº–ç·š
        
        # B-formã®å±é™ºé ˜åŸŸï¼ˆRe(Ï‡) > 0.9ï¼‰ã‚’å¼·èª¿
        if model_form == 'B':
            danger_mask = chi_real > 0.9
            if np.any(danger_mask):
                ax.axhspan(0.9, ax.get_ylim()[1], alpha=0.15, color='red', 
                          label='Danger Zone (Re(Ï‡)>0.9)', zorder=1)
                ax.axhline(0.9, color='orange', linestyle='--', linewidth=1.5, 
                          label='Warning Threshold', alpha=0.7)
        
        # çµ±è¨ˆæƒ…å ±
        chi_real_max = np.max(np.abs(chi_real))
        chi_imag_max = np.max(np.abs(chi_imag))
        
        ax.set_title(f"{data['label']}\nMax|Re(Ï‡)|={chi_real_max:.3f}, Max|Im(Ï‡)|={chi_imag_max:.3f}", 
                    fontsize=10, fontweight='bold')
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Ï‡ (Magnetic Susceptibility)', fontsize=10)
        ax.legend(fontsize=8, loc='best', framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--', linewidth=0.5)
        
        # yè»¸ç¯„å›²ã®è‡ªå‹•èª¿æ•´
        y_margin = max(chi_real_max, chi_imag_max) * 0.1
        y_max = max(chi_real_max, chi_imag_max) * 1.1
        y_min = min(np.min(chi_real), np.min(chi_imag)) - y_margin
        ax.set_ylim(y_min, y_max)
    
    # æœªä½¿ç”¨ã®è»¸ã‚’éè¡¨ç¤º
    for j in range(i+1, len(axes)):
        axes[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'chi_distribution.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ chi_distribution.png saved")

def plot_energy_levels_and_populations(datasets, params_flat, output_dir, model_form):
    """ã‚¨ãƒãƒ«ã‚®ãƒ¼å›ºæœ‰å€¤ã¨å æœ‰ç¢ºç‡ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ8æº–ä½è¡¨ç¤ºï¼‰"""
    global_scaled, gamma_shared = unpack_shared_gamma_parameters(params_flat)
    
    # ç‰©ç†å€¤ã¸ã®å¾©å…ƒ
    g = global_scaled['g'] / SCALING_FACTORS['g']
    a_scale = global_scaled['a'] / SCALING_FACTORS['a']
    B4 = global_scaled['B4'] / SCALING_FACTORS['B4']
    B6 = global_scaled['B6'] / SCALING_FACTORS['B6']
    eps_bg = global_scaled['eps'] / SCALING_FACTORS['eps']
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ãƒ—ãƒ­ãƒƒãƒˆ
    fig_energy, axes_e = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes_e = axes_e.flatten() if n_datasets > 1 else [axes_e]
    
    for i, (data, ax) in enumerate(zip(datasets, axes_e)):
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰
        H_ham = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        E_vals, _ = np.linalg.eigh(H_ham)
        
        # çµ¶å¯¾çš„ãªã‚¨ãƒãƒ«ã‚®ãƒ¼å›ºæœ‰å€¤ã‚’meVã«å¤‰æ›ï¼ˆåŸºæº–å€¤ã‚’å¼•ã‹ãªã„ï¼‰
        E_vals_meV = E_vals * 0.0862  # K â†’ meV
        
        x_pos = np.arange(8)
        
        # ãƒ¢ãƒ‡ãƒ«å½¢å¼ã«å¿œã˜ãŸè‰²
        color = 'red' if model_form == 'H' else 'blue'
        ax.bar(x_pos, E_vals_meV, width=0.6, label=f'{model_form}-form', color=color, alpha=0.7)
        
        ax.set_xlabel('Energy Level Index', fontsize=10)
        ax.set_ylabel('Energy [meV]', fontsize=10)
        ax.set_title(f"{data['label']} - Energy Levels (Absolute)", fontsize=11, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        ax.axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
    
    for j in range(i+1, len(axes_e)):
        axes_e[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f'energy_levels_{model_form}.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ energy_levels_{model_form}.png saved")
    
    # å æœ‰ç¢ºç‡ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ8æº–ä½åˆ¥ã€…ã®ã‚°ãƒ©ãƒ•ï¼‰
    fig_pop, axes_p = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes_p = axes_p.flatten() if n_datasets > 1 else [axes_p]
    
    for i, (data, ax) in enumerate(zip(datasets, axes_p)):
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰
        H_ham = uwbf.get_hamiltonian(data['B'], g, B4, B6)
        E_vals, _ = np.linalg.eigh(H_ham)
        
        # Boltzmannåˆ†å¸ƒã®è¨ˆç®—ã«ã¯ç›¸å¯¾çš„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä½¿ç”¨
        E_vals_rel = E_vals - E_vals.min()
        boltzmann = np.exp(-E_vals_rel / data['T'])
        pops = boltzmann / boltzmann.sum()
        
        x_pos = np.arange(8)
        
        # ãƒ¢ãƒ‡ãƒ«å½¢å¼ã«å¿œã˜ãŸè‰²
        color = 'red' if model_form == 'H' else 'blue'
        ax.bar(x_pos, pops, width=0.6, label=f'{model_form}-form', color=color, alpha=0.7)
        
        ax.set_xlabel('Energy Level Index', fontsize=10)
        ax.set_ylabel('Population', fontsize=10)
        ax.set_title(f"{data['label']} - Populations (T={data['T']:.1f}K)", fontsize=11, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        ax.set_ylim(0, 1.0)
        
        # åŸºåº•çŠ¶æ…‹å æœ‰ç‡ã‚’è¡¨ç¤º
        ax.text(0.02, 0.98, f"Ground state: {pops[0]:.3f}", 
                transform=ax.transAxes, fontsize=9, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    for j in range(i+1, len(axes_p)):
        axes_p[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f'populations_{model_form}.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ populations_{model_form}.png saved")

def plot_energy_levels_and_populations_comparison(datasets, params_H, params_B, output_dir):
    """ã‚¨ãƒãƒ«ã‚®ãƒ¼å›ºæœ‰å€¤ã¨å æœ‰ç¢ºç‡ã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆï¼ˆHå½¢å¼ vs Bå½¢å¼ï¼‰"""
    # Hå½¢å¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    global_H, gamma_H = unpack_shared_gamma_parameters(params_H)
    g_H = global_H['g'] / SCALING_FACTORS['g']
    B4_H = global_H['B4'] / SCALING_FACTORS['B4']
    B6_H = global_H['B6'] / SCALING_FACTORS['B6']
    
    # Bå½¢å¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    global_B, gamma_B = unpack_shared_gamma_parameters(params_B)
    g_B = global_B['g'] / SCALING_FACTORS['g']
    B4_B = global_B['B4'] / SCALING_FACTORS['B4']
    B6_B = global_B['B6'] / SCALING_FACTORS['B6']
    
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    fig_energy, axes_e = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes_e = axes_e.flatten() if n_datasets > 1 else [axes_e]
    
    for i, (data, ax) in enumerate(zip(datasets, axes_e)):
        # Hå½¢å¼
        H_ham_H = uwbf.get_hamiltonian(data['B'], g_H, B4_H, B6_H)
        E_vals_H, _ = np.linalg.eigh(H_ham_H)
        E_vals_H_meV = E_vals_H * 0.0862  # K â†’ meV
        
        # Bå½¢å¼
        H_ham_B = uwbf.get_hamiltonian(data['B'], g_B, B4_B, B6_B)
        E_vals_B, _ = np.linalg.eigh(H_ham_B)
        E_vals_B_meV = E_vals_B * 0.0862  # K â†’ meV
        
        x_pos = np.arange(8)
        width = 0.35
        
        ax.bar(x_pos - width/2, E_vals_H_meV, width, label='H-form', color='red', alpha=0.7)
        ax.bar(x_pos + width/2, E_vals_B_meV, width, label='B-form', color='blue', alpha=0.7)
        
        ax.set_xlabel('Energy Level Index', fontsize=10)
        ax.set_ylabel('Energy [meV]', fontsize=10)
        ax.set_title(f"{data['label']} - Energy Levels (Absolute)", fontsize=11, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        ax.axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
    
    for j in range(i+1, len(axes_e)):
        axes_e[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'energy_levels_HB_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ energy_levels_HB_comparison.png saved")
    
    # å æœ‰ç¢ºç‡æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    fig_pop, axes_p = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes_p = axes_p.flatten() if n_datasets > 1 else [axes_p]
    
    for i, (data, ax) in enumerate(zip(datasets, axes_p)):
        # Hå½¢å¼
        H_ham_H = uwbf.get_hamiltonian(data['B'], g_H, B4_H, B6_H)
        E_vals_H, _ = np.linalg.eigh(H_ham_H)
        E_vals_H_rel = E_vals_H - E_vals_H.min()
        boltzmann_H = np.exp(-E_vals_H_rel / data['T'])
        pops_H = boltzmann_H / boltzmann_H.sum()
        
        # Bå½¢å¼
        H_ham_B = uwbf.get_hamiltonian(data['B'], g_B, B4_B, B6_B)
        E_vals_B, _ = np.linalg.eigh(H_ham_B)
        E_vals_B_rel = E_vals_B - E_vals_B.min()
        boltzmann_B = np.exp(-E_vals_B_rel / data['T'])
        pops_B = boltzmann_B / boltzmann_B.sum()
        
        x_pos = np.arange(8)
        width = 0.35
        
        ax.bar(x_pos - width/2, pops_H, width, label='H-form', color='red', alpha=0.7)
        ax.bar(x_pos + width/2, pops_B, width, label='B-form', color='blue', alpha=0.7)
        
        ax.set_xlabel('Energy Level Index', fontsize=10)
        ax.set_ylabel('Population', fontsize=10)
        ax.set_title(f"{data['label']} - Populations (T={data['T']:.1f}K)", fontsize=11, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        ax.set_ylim(0, 1.0)
        
        # åŸºåº•çŠ¶æ…‹å æœ‰ç‡ã‚’è¡¨ç¤º
        ax.text(0.02, 0.98, f"Ground (H): {pops_H[0]:.3f}\nGround (B): {pops_B[0]:.3f}", 
                transform=ax.transAxes, fontsize=8, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    for j in range(i+1, len(axes_p)):
        axes_p[j].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'populations_HB_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  âœ“ populations_HB_comparison.png saved")

# ==========================================
# ğŸ“Š Main Execution
# ==========================================
def main():
    print("="*80)
    print("Global Fitting v6: Shared Gamma Model (å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…±é€š)")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼š75å€‹ â†’ 12å€‹ï¼ˆ84%å‰Šæ¸›ï¼‰")
    print("="*80)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    datasets = load_all_datasets(TARGET_DATA)
    
    if not datasets:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
        return
    
    # ãƒãƒ©ãƒªãƒˆãƒ³ãƒ¢ãƒ¼ãƒ‰æ¤œå‡ºã¨å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ç”Ÿæˆ
    print("\nğŸ” Detecting polariton modes and assigning frequency-specific weights...")
    for data in datasets:
        # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒãƒ©ãƒªãƒˆãƒ³ã¨å…±æŒ¯å™¨é ˜åŸŸã‚’æ¤œå‡º
        polariton_regions, cavity_regions = detect_peaks_and_classify(data['freq'], data['trans'])
        
        # å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ã‚’åˆæœŸåŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰
        weight_array = np.ones_like(data['freq'])
        
        # ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸã«1.5Ã—ã®é‡ã¿ã‚’é©ç”¨
        for f_start, f_end in polariton_regions:
            mask = (data['freq'] >= f_start) & (data['freq'] <= f_end)
            weight_array[mask] = 1.5
        
        data['weight_array'] = weight_array
        data['polariton_regions'] = polariton_regions
        data['cavity_regions'] = cavity_regions
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        n_polariton = np.sum(weight_array > 1.0)
        n_total = len(weight_array)
        if n_polariton > 0:
            print(f"  âœ“ {data['label']}: {n_polariton}/{n_total} points with polariton weight (1.5Ã—)")
        else:
            print(f"  - {data['label']}: No polariton mode detected (all 1.0Ã—)")
    
    # åˆæœŸå€¤ã¨å¢ƒç•Œå€¤
    global_init, gamma_init = generate_shared_gamma_initial_values()
    params_init_scaled = pack_shared_gamma_parameters(global_init, gamma_init)
    lower_b, upper_b = get_shared_gamma_bounds()
    
    print(f"\nåˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(params_init_scaled)}")
    print(f"  Global: 5 (g, a, B4, B6, eps)")
    print(f"  Shared Gamma: 7 (å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…±é€š)")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(datasets)}")
    print(f"  ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {sum(len(d['freq']) for d in datasets)}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/ãƒ‡ãƒ¼ã‚¿æ¯”: {len(params_init_scaled) / sum(len(d['freq']) for d in datasets) * 100:.2f}%")
    
    # ä¸¡ãƒ¢ãƒ‡ãƒ«å½¢å¼ã®æœ€é©åŒ–çµæœã‚’ä¿å­˜
    results_by_form = {}
    
    for model_form in MODEL_FORMS:
        print(f"\n{'='*80}")
        print(f"ğŸ”„ Model: {model_form}-form")
        print(f"{'='*80}")
        
        # 3æ®µéšæœ€é©åŒ–
        print("\nğŸš€ Stage 1: Quick Exploration...")
        res_stage1 = least_squares(
            lambda p: shared_gamma_residuals(p, datasets, model_form),
            params_init_scaled,
            bounds=(lower_b, upper_b),
            max_nfev=5000,
            ftol=1e-5,
            xtol=1e-5,
            verbose=1
        )
        print(f"  Stage 1 Cost: {res_stage1.cost:.6e}")
        
        print("\nğŸš€ Stage 2: Medium Refinement...")
        res_stage2 = least_squares(
            lambda p: shared_gamma_residuals(p, datasets, model_form),
            res_stage1.x,
            bounds=(lower_b, upper_b),
            max_nfev=15000,
            ftol=1e-7,
            xtol=1e-7,
            verbose=1
        )
        print(f"  Stage 2 Cost: {res_stage2.cost:.6e}")
        
        print("\nğŸš€ Stage 3: Fine Tuning...")
        res_final = least_squares(
            lambda p: shared_gamma_residuals(p, datasets, model_form),
            res_stage2.x,
            bounds=(lower_b, upper_b),
            max_nfev=30000,
            ftol=1e-9,
            xtol=1e-9,
            verbose=2
        )
        print(f"  Final Cost: {res_final.cost:.6e}")
        print(f"  Total improvement: {(1 - res_final.cost/res_stage1.cost)*100:.1f}%")
        
        # çµæœè§£æ
        global_scaled, gamma_shared = unpack_shared_gamma_parameters(res_final.x)
        
        global_phys = {
            'g': global_scaled['g'] / SCALING_FACTORS['g'],
            'a': global_scaled['a'] / SCALING_FACTORS['a'],
            'B4': global_scaled['B4'] / SCALING_FACTORS['B4'],
            'B6': global_scaled['B6'] / SCALING_FACTORS['B6'],
            'eps': global_scaled['eps'] / SCALING_FACTORS['eps']
        }
        gamma_phys = gamma_shared / SCALING_FACTORS['gamma']
        
        print("\n" + "="*80)
        print("âœ… Optimization Complete")
        print("="*80)
        print(f"Final Cost: {res_final.cost:.6e}")
        print(f"Iterations: {res_final.nfev}")
        print("-" * 80)
        print("ã€Global Parametersã€‘")
        for key, val in global_phys.items():
            print(f"  {key:10s}: {val:12.8f}")
        
        print("\nã€Shared Gamma (ææ–™å›ºæœ‰å€¤)ã€‘")
        for i, g in enumerate(gamma_phys, 1):
            print(f"  Î³{i}: {g:.6f} THz")
        
        # æ¡ä»¶æ•°è¨ˆç®—
        print("\n" + "="*80)
        print("ğŸ“ Condition Number Analysis")
        print("="*80)
        try:
            J = res_final.jac
            U, s, Vt = np.linalg.svd(J, full_matrices=False)
            condition_number = s[0] / s[-1] if s[-1] > 1e-15 else np.inf
            
            print(f"  Jacobian shape: {J.shape}")
            print(f"  Parameters: {J.shape[1]} (vs. 75 in v4)")
            print(f"  Max singular value: {s[0]:.4e}")
            print(f"  Min singular value: {s[-1]:.4e}")
            print(f"  Condition number: {condition_number:.4e}")
            
            if condition_number < 1e6:
                print("  âœ… Well-conditioned!")
            elif condition_number < 1e9:
                print("  âš ï¸ Moderately ill-conditioned")
            else:
                print("  âŒ Ill-conditioned")
            
            # v4ã¨ã®æ¯”è¼ƒ
            print(f"\n  ğŸ“Š Comparison:")
            print(f"    v4: Îº â‰ˆ 1.2Ã—10Â¹â¶ (75 params, Cost=27,519)")
            print(f"    v5: Îº = âˆ (21 params, Cost=29,112)")
            print(f"    v6: Îº â‰ˆ {condition_number:.2e} (12 params, Cost={res_final.cost:.0f})")
            
            if condition_number < 1.2e16:
                improvement = 1.2e16 / condition_number
                print(f"    Improvement: {improvement:.2e}Ã—")
            
        except Exception as e:
            print(f"  âš ï¸ Condition number calculation failed: {e}")
            condition_number = None
        
        # ãƒ•ã‚£ãƒƒãƒˆå“è³ª
        fit_stats = analyze_shared_gamma_fit_quality(datasets, res_final.x, model_form)
        print("\n" + "-" * 80)
        print("ã€Fit Quality Statisticsã€‘")
        print(fit_stats.to_string(index=False))
        
        # ç‰©ç†çš„è¨ºæ–­
        diag_df = diagnose_problematic_regions(datasets, res_final.x, model_form)
        
        # çµæœã‚’ä¿å­˜
        results_by_form[model_form] = res_final.x
        
        # å‡ºåŠ›
        out_dir = f"global_fitting_results_{model_form}_v6"
        os.makedirs(out_dir, exist_ok=True)
        
        fit_stats.to_csv(os.path.join(out_dir, 'fit_statistics.csv'), index=False)
        diag_df.to_csv(os.path.join(out_dir, 'diagnostic_analysis.csv'), index=False)
        
        with open(os.path.join(out_dir, 'shared_gamma_params.json'), 'w') as f:
            result_dict = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'model_form': model_form,
                'n_parameters': len(res_final.x),
                'final_cost': float(res_final.cost),
                'condition_number': float(condition_number) if condition_number is not None and np.isfinite(condition_number) else None,
                'global_params': {k: float(v) for k, v in global_phys.items()},
                'shared_gamma': gamma_phys.tolist()
            }
            json.dump(result_dict, f, indent=2)
        
        # ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
        print("\n" + "-" * 80)
        print("ğŸ“Š Generating plots...")
        plot_all_fits(datasets, res_final.x, out_dir, model_form)
        plot_residuals(datasets, res_final.x, out_dir, model_form)
        plot_chi_distribution(datasets, res_final.x, out_dir, model_form)
        
        # æ–°è¦è¿½åŠ : ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã¨å æœ‰ç¢ºç‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
        print("\nğŸ”¬ ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ãƒ»å æœ‰ç¢ºç‡ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")
        plot_energy_levels_and_populations(datasets, res_final.x, out_dir, model_form)
    
    # Hå½¢å¼ã¨Bå½¢å¼ã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆ
    if 'H' in results_by_form and 'B' in results_by_form:
        print("\n" + "="*80)
        print("ğŸ“Š Generating H-form vs B-form comparison plots...")
        print("="*80)
        comparison_dir = "global_fitting_results_comparison_v6"
        os.makedirs(comparison_dir, exist_ok=True)
        
        plot_all_fits_comparison(datasets, results_by_form['H'], results_by_form['B'], comparison_dir)
        
        # æ–°è¦è¿½åŠ : ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã¨å æœ‰ç¢ºç‡ã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        print("\nğŸ”¬ ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ãƒ»å æœ‰ç¢ºç‡æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")
        plot_energy_levels_and_populations_comparison(datasets, results_by_form['H'], results_by_form['B'], comparison_dir)
        
        print(f"\nâœ… Comparison plots saved to: {comparison_dir}/")
        print("="*80)
        
        print(f"\nâœ… Results saved to: {out_dir}/")
        print("="*80)

if __name__ == "__main__":
    main()
