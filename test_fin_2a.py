"""
Bayesian Hierarchical Analysis with SMC Sampling (v7.1)
Non-centeredéšå±¤Î³ãƒ¢ãƒ‡ãƒ« + SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + ç‰©ç†çš„åˆ¶ç´„ãƒ™ãƒ¼ã‚¹äº‹å‰åˆ†å¸ƒ

ã€v7.1æ›´æ–°ã€‘2026-01-18
â˜… Non-centered Parameterizationã¸ç§»è¡Œï¼ˆR-hatåæŸæ€§æ”¹å–„ï¼‰
  - Centeredç‰ˆ: Î³_i ~ N(Î³_mean, Î³_std) â†’ ã€Œæ¼æ–—ã€å•é¡Œã§åæŸä¸è‰¯
  - Non-centeredç‰ˆ: z_i ~ N(0,1), Î³_i = exp(log_Î¼ + log_Ïƒ * z_i)
  - æœŸå¾…åŠ¹æœ: R-hat 1.135â†’<1.01, ESS 8396â†’>25000

ã€v7æ›´æ–°ã€‘2026-01-14
1. éšå±¤çš„Î³ãƒ¢ãƒ‡ãƒ«å°å…¥: Î³è­˜åˆ¥ä¸èƒ½æ€§ã‚’è§£æ¶ˆ
2. SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼ˆESS>400ç›®æ¨™ï¼‰
3. ç‰©ç†çš„åˆ¶ç´„ãƒ™ãƒ¼ã‚¹äº‹å‰åˆ†å¸ƒ: v6çµæœã¯å‚è€ƒå€¤ã®ã¿
4. å¤–ã‚Œå€¤é ‘å¥å°¤åº¦: StudentTåˆ†å¸ƒï¼ˆÎ½=4ï¼‰
5. é‡ã¿è¨­å®šå¤‰æ›´: ãƒãƒ©ãƒªãƒˆãƒ³=2.0, å…±æŒ¯å™¨=1.0, ãã®ä»–=0.01ï¼ˆv7.1ã§0.1â†’0.01ã«å¤‰æ›´ï¼‰

ã€äº‹å‰åˆ†å¸ƒè¨­å®šï¼ˆv7.1 Non-centeredç‰ˆï¼‰ã€‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿â”‚åˆ†å¸ƒå‹        â”‚è¨­å®šæ ¹æ‹                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ g       â”‚TruncNormal   â”‚ç†è«–å€¤gâ‰ˆ2.0 (GdÂ³âº), Ïƒ=0.05             â”‚
â”‚ a       â”‚HalfNormal    â”‚ä½å€¤å„ªå…ˆã€ä¸Šé™10ã«æ‹¡å¼µï¼ˆv6å¼µã‚Šä»˜ãå¯¾å¿œï¼‰â”‚
â”‚ Bâ‚„      â”‚LogNormal     â”‚æ­£å€¤ä¿è¨¼ã€ä½å€¤å„ªå…ˆã€ä¸Šé™50mKã«æ‹¡å¼µ      â”‚
â”‚ Bâ‚†      â”‚Normal        â”‚ã‚¼ãƒ­ä¸­å¿ƒå¯¾ç§°ã€ç¯„å›²[-2mK, +2mK]         â”‚
â”‚ Îµ_bg   â”‚TruncNormal   â”‚v6å¹³å‡å€¤ä¸­å¿ƒã€Ïƒ=0.3ï¼ˆæƒ…å ±å¼·åŒ–ï¼‰        â”‚
â”‚log_Î³_muâ”‚Normal        â”‚logç©ºé–“ã§å®šç¾©ã€Î¼=log(0.074)            â”‚
â”‚log_Î³_sdâ”‚HalfNormal    â”‚logç©ºé–“æ¨™æº–åå·®ã€Ïƒ=0.5                 â”‚
â”‚Î³_raw_i â”‚Normal(0,1)   â”‚Non-centered: æ¨™æº–æ­£è¦åˆ†å¸ƒï¼ˆç‹¬ç«‹ï¼‰      â”‚
â”‚ Î³_i    â”‚Deterministic â”‚exp(log_Î¼ + log_Ïƒ * z_i) (æ±ºå®šè«–çš„å¤‰æ›) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€æ©Ÿèƒ½ã€‘
1. Hå½¢å¼ã¨Bå½¢å¼ã‚’ä¸¡æ–¹åŒæ™‚ã«å‡¦ç†
2. SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆDraws=5000, Chains=8ï¼‰
3. é‡ã¿è¨­å®š: ãƒãƒ©ãƒªãƒˆãƒ³=2.0, é«˜æ¬¡å…±æŒ¯å™¨=1.0, ãã‚Œä»¥å¤–=0.01
4. LOO-CV (Leave-One-Out Cross-Validation): ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨æ¯”è¼ƒ
5. ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: DEBUG_MODE=Trueã§é«˜é€Ÿãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
"""

# ========== è¨­å®šï¼ˆv7éšå±¤ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰==========
SAMPLER_TYPE = 'SMC'              # SMCã§é–‹å§‹ï¼ˆæœ€ã‚‚å®‰å®šï¼‰
USE_HIERARCHICAL_GAMMA = True     # éšå±¤Î³ãƒ¢ãƒ‡ãƒ«å¿…é ˆ
USE_V6_AS_REFERENCE_ONLY = True   # v6ã¯å‚è€ƒå€¤ã®ã¿ï¼ˆä¸­å¿ƒå€¤ã«ã¯ä½¿ã‚ãªã„ï¼‰
LIKELIHOOD_TYPE = 'studentt'      # å¤–ã‚Œå€¤é ‘å¥æ€§
NU_STUDENTT = 4                   # è‡ªç”±åº¦
RANDOM_SEED = 42                   # ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰

# SMCè¨­å®šï¼ˆESS>400é”æˆå‘ã‘ï¼‰
SMC_DRAWS = 10000      # é«˜å“è³ªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
SMC_CHAINS = 16        # ä¸¦åˆ—åº¦å‘ä¸Š
SMC_PARALLEL = True

# éšå±¤Î³ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆWNLSæœ€é©åŒ–çµæœãƒ™ãƒ¼ã‚¹: H/Bå½¢å¼140ã‚µãƒ³ãƒ—ãƒ«åˆ†æï¼‰
GAMMA_HYPERPRIOR_MU = 0.074    # ä¸­å¤®å€¤ï¼ˆå¤–ã‚Œå€¤ãƒ­ãƒã‚¹ãƒˆï¼‰
GAMMA_HYPERPRIOR_SIGMA = 0.160  # å…¨ä½“æ¨™æº–åå·®Ã—1.5ï¼ˆåºƒã„æ¢ç´¢ç¯„å›²ï¼‰
GAMMA_STD_PRIOR = 0.092         # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã°ã‚‰ã¤ãï¼ˆå€‹åˆ¥Î³ã®å¤‰å‹•è¨±å®¹ï¼‰

import os
import sys
import json
import time
import pathlib
import datetime
import warnings
warnings.filterwarnings('ignore')

# CPUç’°å¢ƒè¨­å®š
os.environ['OMP_NUM_THREADS'] = '8'
os.environ['MKL_NUM_THREADS'] = '8'
os.environ['OPENBLAS_NUM_THREADS'] = '8'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans',


import pymc as pm
import arviz as az
import pytensor
import pytensor.tensor as pt
from pytensor.graph.basic import Apply
from pytensor.graph.op import Op
from scipy.signal import find_peaks
from scipy.stats import truncnorm

# PyTensorã®è­¦å‘Šã‚’æŠ‘åˆ¶
import logging
logging.getLogger('pytensor').setLevel(logging.ERROR)

# å®šæ•°å®šç¾©ï¼ˆunified_weighted_bayesian_fitting_final.pyæº–æ‹ ï¼‰
kB = 1.380649e-23      # ãƒœãƒ«ãƒ„ãƒãƒ³å®šæ•° [J/K]
muB = 9.274010e-24     # ãƒœãƒ¼ã‚¢ç£å­ [J/T]
hbar = 1.054571e-34    # ãƒ—ãƒ©ãƒ³ã‚¯å®šæ•° [JÂ·s]
c = 299792458          # å…‰é€Ÿ [m/s]
mu0 = 4.0 * np.pi * 1e-7  # çœŸç©ºé€ç£ç‡ [H/m]
eps0 = 8.854187817e-12    # çœŸç©ºèª˜é›»ç‡ [F/m]

# THzå˜ä½ç³»å¤‰æ›å®šæ•°
THZ_TO_HZ = 1e12
THZ_TO_RAD_S = 2.0 * np.pi * THZ_TO_HZ  # THz â†’ rad/s
RAD_S_TO_THZ = 1.0 / THZ_TO_RAD_S        # rad/s â†’ THz

# ã‚¹ãƒ”ãƒ³å¯†åº¦ã¨è©¦æ–™åšã•ï¼ˆpre_test_v6_shared_gamma.pyæº–æ‹ ï¼‰
N_SPIN = 1.9386e+28    # ã‚¹ãƒ”ãƒ³å¯†åº¦ [m^-3]
d_fixed = 157.8e-6     # è©¦æ–™åšã• [m]

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆv7éšå±¤ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
# ç›®æ¨™: æœ€é©åŒ–ç©ºé–“ã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¹…ã‚’50ç¨‹åº¦ã«çµ±ä¸€
# v7å¢ƒç•Œæ‹¡å¼µç‰ˆ: a=[0.1, 10.0], Bâ‚„=[0.01mK, 50mK], Bâ‚†=[-2mK, 2mK]
SCALING_FACTORS = {
    'g': 38.0,      # [1.5, 2.8] â†’ [57, 106] (å¹…49)
    'a': 10.2,      # [0.1, 10.0] â†’ [1.02, 102.0] (v7: ä¸Šé™10ã«æ‹¡å¼µ)
    'B4': 1672.0,   # [1e-5, 5e-2] â†’ [0.017, 83.6] (v7: ä¸Šé™50mKã«æ‹¡å¼µ)
    'B6': 25000.0,  # [-2e-3, 2e-3] â†’ [-50, 50] (v7: ç¯„å›²æ‹¡å¼µ)
    'eps': 17.0,    # [13.0, 16.0] â†’ [221, 272] (å¹…51)
    'gamma': 100.0  # [0.005, 0.5] â†’ [0.5, 50.0] (v7: ä¸‹é™ç·©å’Œ)
}

# å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
TARGET_DATA = [
    {'B': 9.0, 'T': 4.0,  'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '4K'},
    {'B': 9.0, 'T': 10.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '10K'},
    {'B': 9.0, 'T': 20.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '20K'},
    {'B': 9.0, 'T': 30.0, 'file': 'BayesianInput_Raw_Transmittance_Temperature.xlsx', 'sheet': 'Normalized Data', 'col': '30K'},
    {'B': 4.2, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '4.2T'},
    {'B': 5.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '5T'},
    {'B': 6.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '6T'},
    {'B': 7.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '7T'},
    {'B': 8.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '8T'},
    {'B': 9.0, 'T': 1.5, 'file': 'BayesianInput_Raw_Transmittance_Field.xlsx', 'sheet': 'Normalized Data', 'col': '9T'},
]

# ============================================================================
# ç‰©ç†é–¢æ•°ç¾¤ï¼ˆunified_weighted_bayesian_fitting_final.pyæº–æ‹ ï¼‰
# ============================================================================

# ã‚¹ãƒ”ãƒ³é‡å­æ•°ã¨ã‚¬ãƒ³ãƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
S_VALUE = 3.5
N_TRANSITIONS = 7  # 7-gammaåˆæœŸçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹æ–¹å¼

def get_hamiltonian(B_ext_z, g_factor, B4, B6, s=S_VALUE):
    """ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆStevensæ¼”ç®—å­ä½¿ç”¨ï¼‰"""
    n_states = int(2 * s + 1)
    m_values = np.arange(s, -s - 1, -1)
    Sz = np.diag(m_values)
    
    if n_states == 8:  # s = 7/2 (Gd3+)
        # Stevensæ¼”ç®—å­ O40, O44ï¼ˆæ­£è¦åŒ–ï¼šç„¡æ¬¡å…ƒï¼‰
        O40 = np.diag([7, -13, -3, 9, 9, -3, -13, 7]) / 60
        X_O44 = np.zeros((8, 8))
        X_O44[3, 7] = X_O44[4, 0] = np.sqrt(35) / 12
        X_O44[2, 6] = X_O44[5, 1] = 5 * np.sqrt(3) / 12
        O44 = (X_O44 + X_O44.T)
        
        # Stevensæ¼”ç®—å­ O60, O64ï¼ˆæ­£è¦åŒ–ï¼šç„¡æ¬¡å…ƒï¼‰
        O60 = np.diag([1, -5, 9, -5, -5, 9, -5, 1]) / 1260
        X_O64 = np.zeros((8, 8))
        X_O64[3, 7] = X_O64[4, 0] = 3 * np.sqrt(35) / 60
        X_O64[2, 6] = X_O64[5, 1] = -7 * np.sqrt(3) / 60
        O64 = (X_O64 + X_O64.T)
    else:
        raise ValueError(f"s={s}ã®çµæ™¶å ´æ¼”ç®—å­ã¯æœªå®Ÿè£…ã§ã™")
    
    # çµæ™¶å ´ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ï¼ˆKå˜ä½ï¼‰
    H_cf = B4 * (O40 + 5 * O44) + B6 * (O60 - 21 * O64)
    
    # ã‚¼ãƒ¼ãƒãƒ³é …ï¼ˆJouleå˜ä½ã‹ã‚‰Kå˜ä½ã«å¤‰æ›ï¼‰
    H_zee_J = g_factor * muB * B_ext_z * Sz  # Joule
    H_zee = H_zee_J / kB  # K
    
    return H_cf + H_zee


def construct_spin_operators():
    """S=7/2ç³»ã®ã‚¹ãƒ”ãƒ³æ¼”ç®—å­ Sx, Sy, Sz ã®è¡Œåˆ—ã‚’æ§‹ç¯‰"""
    s_val = 3.5
    n_states = int(2 * s_val + 1)  # 8æº–ä½
    m_values = np.arange(s_val, -s_val - 1, -1)  # [7/2, 5/2, ..., -7/2]
    
    # Sz ã¯å¯¾è§’è¡Œåˆ—
    Sz = np.diag(m_values)
    
    # S+ ã¨ S- ã®è¡Œåˆ—è¦ç´ ã‚’è¨ˆç®—
    Sx = np.zeros((n_states, n_states), dtype=float)
    Sy = np.zeros((n_states, n_states), dtype=float)
    
    # S+ ã®è¡Œåˆ—è¦ç´ ï¼ˆå¯¾è§’ã®1ã¤ä¸Šï¼‰
    for i in range(n_states - 1):
        m_lower = m_values[i + 1]
        coeff = np.sqrt((s_val - m_lower) * (s_val + m_lower + 1))
        Sx[i, i + 1] += coeff / 2.0
        Sy[i, i + 1] += -coeff / (2.0j)
    
    # S- ã®è¡Œåˆ—è¦ç´ ï¼ˆå¯¾è§’ã®1ã¤ä¸‹ï¼‰
    for i in range(1, n_states):
        m_upper = m_values[i - 1]
        coeff = np.sqrt((s_val + m_upper) * (s_val - m_upper + 1))
        Sx[i, i - 1] += coeff / 2.0
        Sy[i, i - 1] += coeff / (2.0j)
    
    # Syã¯è¤‡ç´ æ•°è¡Œåˆ—ã ãŒã€è™šéƒ¨ã‚’å®Ÿæ•°ã¨ã—ã¦æ‰±ã†ï¼ˆSy = -i(S+ - S-)/2 ã®å®Ÿéƒ¨ï¼‰
    # ç‰©ç†çš„ã«æ­£ã—ã„å®Ÿæ•°Syè¡Œåˆ—ã‚’è¿”ã™
    Sy_real = np.imag(Sy)  # è™šéƒ¨ãŒå®Ÿéš›ã®Syæˆåˆ†
    return Sx, Sy_real, Sz


def calculate_susceptibility(freq_thz, H, T, gamma_thz):
    """ç£æ°—æ„Ÿå—ç‡ã‚’å³å¯†ã«è¨ˆç®—ï¼ˆå…¨56é·ç§»è€ƒæ…®ã€7-gammaåˆæœŸçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹æ–¹å¼ï¼‰"""
    # Î³ã®å‡¦ç†
    if np.isscalar(gamma_thz):
        gamma_mode = 'uniform'
        gamma_uniform = float(gamma_thz)
        gamma_array_7 = None
    elif hasattr(gamma_thz, '__len__'):
        gamma_array = np.atleast_1d(gamma_thz)
        if len(gamma_array) == 7:
            gamma_mode = '7gamma'
            gamma_array_7 = gamma_array
        else:
            gamma_mode = 'uniform'
            gamma_uniform = float(gamma_array[0])
            gamma_array_7 = None
    else:
        gamma_mode = 'uniform'
        gamma_uniform = float(gamma_thz)
        gamma_array_7 = None
    
    # 1. ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’å¯¾è§’åŒ–
    eigenvalues_K, eigenvectors = np.linalg.eigh(H)
    E_min = np.min(eigenvalues_K)
    E_shifted_K = eigenvalues_K - E_min  # [K]
    
    # 2. Boltzmannå› å­è¨ˆç®—
    boltzmann_exp = np.clip(E_shifted_K / T, -700, 700)
    Z = np.sum(np.exp(-boltzmann_exp))
    populations = np.exp(-boltzmann_exp) / Z
    
    # 3. ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã‚’Jå˜ä½ã«å¤‰æ›
    E_shifted_J = E_shifted_K * kB  # [J]
    
    # 4. ã‚¹ãƒ”ãƒ³æ¼”ç®—å­ã®æ§‹ç¯‰
    Sx_zeeman, Sy_zeeman, Sz_zeeman = construct_spin_operators()
    
    # 5. çµæ™¶å ´å›ºæœ‰çŠ¶æ…‹ã§ã®é·ç§»è¡Œåˆ—è¦ç´ ã‚’è¨ˆç®—
    Sx_eigenbasis = eigenvectors.T.conj() @ Sx_zeeman @ eigenvectors
    Sy_eigenbasis = eigenvectors.T.conj() @ Sy_zeeman @ eigenvectors
    Sz_eigenbasis = eigenvectors.T.conj() @ Sz_zeeman @ eigenvectors
    
    # ç£æ°—æ„Ÿå—ç‡ãƒ†ãƒ³ã‚½ãƒ«ã®å¯¾è§’æˆåˆ†
    transition_xx = np.abs(Sx_eigenbasis)**2
    transition_yy = np.abs(Sy_eigenbasis)**2
    transition_zz = np.abs(Sz_eigenbasis)**2
    
    # 6. å…¨é·ç§»ãƒšã‚¢ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼å·®ã‚’è¨ˆç®—
    delta_E_matrix = E_shifted_J[None, :] - E_shifted_J[:, None]  # (8, 8), [J]
    omega_0_rad = delta_E_matrix / hbar  # [rad/s]
    freq_0_matrix = omega_0_rad * RAD_S_TO_THZ  # [THz]
    
    # 7. å®ŸåŠ¹çš„ãªé·ç§»å¼·åº¦ï¼ˆé¢å†…å¹³å‡ï¼‰
    transition_perp = (transition_xx + transition_yy) / 2.0
    
    # å æœ‰ç¢ºç‡å·®åˆ†
    pop_diff_matrix = populations[:, None] - populations[None, :]
    
    # Boltzmanné‡ã¿ä»˜ãé·ç§»å¼·åº¦
    strength_matrix = pop_diff_matrix * transition_perp
    
    # 8. å¯¾è§’è¦ç´ ã‚’é™¤å¤–
    non_diag_mask = ~np.eye(8, dtype=bool)
    
    # ä½æ¸©ã§ã®åˆ†è£‚æ§‹é€ ä¿æŒ
    population_threshold = 1e-3
    occupied_mask = populations[:, None] > population_threshold
    
    # æœ‰é™å€¤ãƒã‚§ãƒƒã‚¯
    finite_mask = (
        np.isfinite(freq_0_matrix) & 
        np.isfinite(strength_matrix) & 
        (np.abs(strength_matrix) > 1e-20) &
        occupied_mask &
        non_diag_mask
    )
    
    if not np.any(finite_mask):
        return np.zeros_like(freq_thz, dtype=complex)
    
    # 9. æœ‰åŠ¹ãªé·ç§»ã®ã¿æŠ½å‡º
    freq_0_valid = freq_0_matrix[finite_mask]
    strength_valid = strength_matrix[finite_mask]
    
    # Î³å€¤ã®å‰²ã‚Šå½“ã¦ï¼ˆé·ç§»ã”ã¨ï¼‰
    n_indices, n_prime_indices = np.where(finite_mask)
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã§ã‚½ãƒ¼ãƒˆ
    energy_order = np.argsort(E_shifted_J)
    
    if gamma_mode == 'uniform':
        gamma_per_transition = np.full(len(freq_0_valid), gamma_uniform)
    elif gamma_mode == '7gamma':
        # 7-gamma: åˆæœŸçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹æ–¹å¼
        gamma_per_transition = np.zeros(len(freq_0_valid))
        
        for trans_idx in range(len(freq_0_valid)):
            n = n_indices[trans_idx]
            n_prime = n_prime_indices[trans_idx]
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ä½ã„æ–¹ã®æº–ä½ã‚’å–å¾—
            E_n = E_shifted_J[n]
            E_n_prime = E_shifted_J[n_prime]
            
            if E_n <= E_n_prime:
                lower_state = n
            else:
                lower_state = n_prime
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼é †ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            lower_state_energy_idx = np.where(energy_order == lower_state)[0][0]
            
            # å¯¾å¿œã™ã‚‹Î³ã‚’é¸æŠï¼ˆæº–ä½0ã€œ6ã«å¯¾å¿œã™ã‚‹Î³_0ã€œÎ³_6ï¼‰
            gamma_idx = min(lower_state_energy_idx, 6)
            gamma_per_transition[trans_idx] = gamma_array_7[gamma_idx]
    else:
        gamma_per_transition = np.full(len(freq_0_valid), 0.1)
    
    # 10. æ„Ÿå—ç‡è¨ˆç®—
    freq_diff = freq_0_valid[None, :] - freq_thz[:, None]
    denominator = freq_diff - 1j * gamma_per_transition[None, :]
    
    # ã‚¼ãƒ­é™¤ç®—å›é¿
    safe_mask = np.abs(denominator) > 1e-10
    denominator = np.where(safe_mask, denominator, 1e-10 + 1j * 1e-10)
    
    # å„å‘¨æ³¢æ•°ã«å¯¾ã™ã‚‹å…¨é·ç§»ã®å¯„ä¸ã‚’åˆè¨ˆ
    chi_array = np.sum(strength_valid[None, :] / denominator, axis=1)
    
    return chi_array


def calculate_transmission(freq_thz, mu_r, d, eps_bg):
    """é€éç‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆFabry-Perotå¹²æ¸‰è€ƒæ…®ï¼‰"""
    eps_bg = max(eps_bg, 0.1)
    d = max(d, 1e-6)
    
    omega = freq_thz * THZ_TO_RAD_S
    
    # æ¯”é€ç£ç‡ã®å®‰å…¨å‡¦ç†
    mu_r_safe = np.where(np.isfinite(mu_r), mu_r, 1.0 + 0j)
    eps_mu = eps_bg * mu_r_safe
    eps_mu = np.where(eps_mu.real > 0, eps_mu, 0.1 + 1j * eps_mu.imag)
    
    # è¤‡ç´ å±ˆæŠ˜ç‡ã¨è¤‡ç´ ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹
    n_complex = np.sqrt(eps_mu + 0j)
    impe = np.sqrt(mu_r_safe / eps_bg + 0j)
    
    # æ³¢é•·ã¨ä½ç›¸
    lambda_0 = np.where(omega > 1e-12, (2 * np.pi * c) / omega, np.inf)
    delta = 2 * np.pi * n_complex * d / lambda_0
    delta = np.clip(delta.real, -700, 700) + 1j * np.clip(delta.imag, -700, 700)
    
    # Fabry-Peroté€éç‡
    numerator = 4 * impe
    exp_pos = np.exp(-1j * delta)
    exp_neg = np.exp(1j * delta)
    denominator = (1 + impe)**2 * exp_pos - (1 - impe)**2 * exp_neg
    
    safe_mask = np.abs(denominator) > 1e-15
    t = np.zeros_like(denominator, dtype=complex)
    t[safe_mask] = numerator[safe_mask] / denominator[safe_mask]
    
    transmission = np.abs(t)**2
    transmission = np.where(np.isfinite(transmission), transmission, 0.0)
    transmission = np.clip(transmission, 0, 2)
    
    # Min-Maxæ­£è¦åŒ–
    t_min, t_max = np.min(transmission), np.max(transmission)
    if t_max > t_min and np.isfinite(t_max) and np.isfinite(t_min):
        normalized = (transmission - t_min) / (t_max - t_min)
        return np.clip(normalized, 0.0, 1.0)
    else:
        return np.full_like(transmission, 0.5)


# ============================================================================
# v6çµæœèª­ã¿è¾¼ã¿
# ============================================================================
def load_v6_optimized_params(model_form='H'):
    """v6æœ€é©åŒ–çµæœã®èª­ã¿è¾¼ã¿ï¼ˆpre_test_v6_shared_gamma.pyå½¢å¼ï¼‰"""
    json_path = pathlib.Path(__file__).parent / f"global_fitting_results_{model_form}_v6" / "shared_gamma_params.json"
    
    if not json_path.exists():
        print(f"âŒ {json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # pre_test_v6ã®JSONæ§‹é€ ã«å¯¾å¿œ
        global_params = data['global_params']
        
        params = {
            'g': global_params['g'],
            'a': global_params['a'],
            'B4': global_params['B4'],
            'B6': global_params['B6'],
            'eps': global_params['eps'],
            'gamma': np.array(data['shared_gamma']),
            'cost': data.get('final_cost', None),
            'condition_number': data.get('condition_number', None)
        }
        
        print(f"\nâœ“ {model_form}-form v6æœ€é©åŒ–çµæœ:")
        print(f"  g = {params['g']:.6f}")
        print(f"  a = {params['a']:.6f}")
        print(f"  B4 = {params['B4']:.8f}")
        print(f"  B6 = {params['B6']:.8f}")
        print(f"  eps = {params['eps']:.6f}")
        print(f"  gamma = {params['gamma']}")
        if params['cost']:
            print(f"  cost = {params['cost']:.1f}")
        if params['condition_number']:
            print(f"  Îº = {params['condition_number']:.2e}")
        
        return params
        
    except Exception as e:
        print(f"âŒ {model_form}-formèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# ============================================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================================================================
def detect_peaks_and_classify(freq, trans, polariton_upper=0.361505, cavity_lower=0.45):
    """ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨ãƒãƒ©ãƒªãƒˆãƒ³/å…±æŒ¯å™¨åˆ†é¡ï¼ˆé€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼‰"""
    # é€éç‡ã®æ¥µå¤§å€¤ã‚’æ¤œå‡ºï¼ˆpre_test_v6ã¨åŒæœŸï¼‰
    peaks, properties = find_peaks(trans, prominence=0.05, width=3)
    
    if len(peaks) == 0:
        return [], []
    
    peak_freqs = freq[peaks]
    peak_widths = properties['widths'] * (freq[1] - freq[0])
    
    sort_idx = np.argsort(peak_freqs)
    peak_freqs = peak_freqs[sort_idx]
    peak_widths = peak_widths[sort_idx]
    
    polariton_regions = []
    cavity_regions = []
    
    for pf, pw in zip(peak_freqs, peak_widths):
        f_start = max(freq[0], pf - 1.5 * pw)
        f_end = min(freq[-1], pf + 1.5 * pw)
        
        if pf <= polariton_upper:
            f_end_clipped = min(f_end, polariton_upper)
            if f_end_clipped > f_start:
                polariton_regions.append((f_start, f_end_clipped))
        elif pf >= cavity_lower:
            f_start_clipped = max(f_start, cavity_lower)
            if f_end > f_start_clipped:
                cavity_regions.append((f_start_clipped, f_end))
    
    return polariton_regions, cavity_regions


def create_weight_array(freq, trans, polariton_regions, cavity_regions):
    """é‡ã¿é…åˆ—ç”Ÿæˆ: ãƒãƒ©ãƒªãƒˆãƒ³=2.0, å…±æŒ¯å™¨=1.0, ãã‚Œä»¥å¤–=0.01ï¼ˆv7æ›´æ–°ï¼‰"""
    weight_array = np.full_like(freq, 0.01)  
    
    # ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸ: 2.0ï¼ˆ1.5ã‹ã‚‰å¤‰æ›´ï¼‰
    for f_start, f_end in polariton_regions:
        mask = (freq >= f_start) & (freq <= f_end)
        weight_array[mask] = 2.0
    
    # å…±æŒ¯å™¨é ˜åŸŸ: 1.0
    for f_start, f_end in cavity_regions:
        mask = (freq >= f_start) & (freq <= f_end)
        weight_array[mask] = 1.0
    
    return weight_array


def load_all_datasets(target_data_list):
    """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆpre_test_v6äº’æ›ï¼‰"""
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---")
    
    datasets = []
    base_dir = pathlib.Path(__file__).parent / 'bayesian_inputs'
    
    for idx, config in enumerate(target_data_list):
        excel_path = base_dir / config['file']
        
        if not excel_path.exists():
            print(f"âŒ {excel_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue
        
        try:
            # Excelã‚·ãƒ¼ãƒˆã¨åˆ—åã‹ã‚‰èª­ã¿è¾¼ã¿
            df = pd.read_excel(excel_path, sheet_name=config['sheet'])
            
            # å‘¨æ³¢æ•°åˆ—ï¼ˆå…±é€šï¼‰
            if 'Frequency (THz)' not in df.columns:
                print(f"âŒ {config['col']}: å‘¨æ³¢æ•°åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            # ãƒ‡ãƒ¼ã‚¿åˆ—ï¼ˆå„æ¡ä»¶ï¼‰
            if config['col'] not in df.columns:
                print(f"âŒ {config['col']}: ãƒ‡ãƒ¼ã‚¿åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            df_clean = df[['Frequency (THz)', config['col']]].dropna()
            freq = df_clean['Frequency (THz)'].values.astype(np.float64)
            trans = df_clean[config['col']].values.astype(np.float64)
            
            # å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ç”Ÿæˆ
            polariton_regions, cavity_regions = detect_peaks_and_classify(freq, trans)
            weight_array = create_weight_array(freq, trans, polariton_regions, cavity_regions)
            
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆB/Tæ¡ä»¶ï¼‰
            if config['T'] == 1.5:
                label = f"{config['B']:.1f}T"
            else:
                label = f"{config['T']:.0f}K"
            
            dataset = {
                'freq': freq,
                'trans': trans,
                'weight': weight_array,
                'B': config['B'],
                'T': config['T'],
                'label': label,
                'polariton_regions': polariton_regions,
                'cavity_regions': cavity_regions,
                'sigma': np.full_like(freq, 0.01)  # å‡ä¸€ãªãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            }
            
            datasets.append(dataset)
            
            print(f"âœ“ {label} (B={config['B']}T, T={config['T']}K): {len(freq)} points")
            print(f"  Polaritoné ˜åŸŸ (2.0Ã—): {len(polariton_regions)} regions")
            print(f"  Cavityé ˜åŸŸ (1.0Ã—): {len(cavity_regions)} regions")
            
        except Exception as e:
            print(f"âŒ {config['col']} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if len(datasets) == 0:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ1ã¤ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print(f"\nâœ… åˆè¨ˆ {len(datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
    
    return datasets


# ============================================================================
# PyTensor Op (ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œç‰ˆ)
# ============================================================================
class ScaledInformedPriorModelOp(Op):
    """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œã®æƒ…å ±çš„äº‹å‰åˆ†å¸ƒãƒ¢ãƒ‡ãƒ«Opï¼ˆH/Bå½¢å¼é¸æŠå¯èƒ½ï¼‰"""
    
    def __init__(self, datasets, model_form='H'):
        self.datasets = datasets
        self.model_form = model_form
    
    def make_node(self, a_scale_scaled, gamma_vec_scaled, g_factor_scaled, 
                  B4_scaled, B6_scaled, eps_bg_scaled):
        a_scale_scaled = pt.as_tensor_variable(a_scale_scaled)
        gamma_vec_scaled = pt.as_tensor_variable(gamma_vec_scaled)
        g_factor_scaled = pt.as_tensor_variable(g_factor_scaled)
        B4_scaled = pt.as_tensor_variable(B4_scaled)
        B6_scaled = pt.as_tensor_variable(B6_scaled)
        eps_bg_scaled = pt.as_tensor_variable(eps_bg_scaled)
        
        n_total = sum(len(d['freq']) for d in self.datasets)
        output = pt.dvector()
        
        return Apply(self, 
                    [a_scale_scaled, gamma_vec_scaled, g_factor_scaled, 
                     B4_scaled, B6_scaled, eps_bg_scaled],
                    [output])
    
    def perform(self, node, inputs, output_storage):
        a_scale_scaled, gamma_vec_scaled, g_factor_scaled, B4_scaled, B6_scaled, eps_bg_scaled = inputs
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰©ç†å€¤ã«å¤‰æ›
        g_factor = g_factor_scaled / SCALING_FACTORS['g']
        a_scale = a_scale_scaled / SCALING_FACTORS['a']
        B4 = B4_scaled / SCALING_FACTORS['B4']
        B6 = B6_scaled / SCALING_FACTORS['B6']
        eps_bg = eps_bg_scaled / SCALING_FACTORS['eps']
        
        gamma_array_scaled = np.atleast_1d(gamma_vec_scaled).astype(np.float64)
        if len(gamma_array_scaled) != 7:
            gamma_array_scaled = np.full(7, gamma_array_scaled[0])
        gamma_array = gamma_array_scaled / SCALING_FACTORS['gamma']
        
        all_trans_pred = []
        
        for data in self.datasets:
            freq = data['freq']
            B = data['B']
            T = data['T']
            
            H_ham = get_hamiltonian(B, g_factor, B4, B6)
            chi_raw = calculate_susceptibility(freq, H_ham, T, gamma_array)
            
            G0 = a_scale * mu0 * N_SPIN * (g_factor * muB)**2 / (2 * hbar) / THZ_TO_RAD_S
            chi = G0 * chi_raw
            
            if self.model_form == 'H':
                # Hå½¢å¼: Î¼r = 1 + Ï‡
                mu_r = 1.0 + chi
            else:
                # Bå½¢å¼: Î¼r = 1 / (1 - Ï‡)
                denominator = 1.0 - chi
                mu_r = 1.0 / denominator
            
            trans_pred = calculate_transmission(freq, mu_r, d_fixed, eps_bg)
            
            all_trans_pred.append(trans_pred)
        
        output_storage[0][0] = np.concatenate(all_trans_pred)


# ============================================================================
# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–¢æ•°ï¼ˆSMCå¯¾å¿œ: WAICãƒ™ãƒ¼ã‚¹ï¼‰
# ============================================================================
def compute_model_evaluation(trace, model_name='Model'):
    """
    ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆSMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯¾å¿œï¼‰
    
    SMCã¯log_likelihoodã‚’è‡ªå‹•ä¿å­˜ã—ãªã„ãŸã‚ã€WAICã‚’ä½¿ç”¨ã€‚
    WAICãŒè¨ˆç®—ã§ããªã„å ´åˆã¯äº‹å¾Œäºˆæ¸¬ã‚µãƒãƒªãƒ¼ã‚’è¿”ã™ã€‚
    """
    print(f"\n{'='*80}")
    print(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡: {model_name}")
    print(f"{'='*80}")
    
    result = {'model_name': model_name}
    
    # 1. åŸºæœ¬çµ±è¨ˆé‡
    try:
        summary = az.summary(trace)
        n_params = len(summary)
        mean_rhat = summary['r_hat'].mean() if 'r_hat' in summary.columns else np.nan
        mean_ess = summary['ess_bulk'].mean() if 'ess_bulk' in summary.columns else np.nan
        
        print(f"\nğŸ“Š {model_name} ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµ±è¨ˆ:")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {n_params}")
        if not np.isnan(mean_rhat):
            print(f"  å¹³å‡R-hat: {mean_rhat:.4f}")
        if not np.isnan(mean_ess):
            print(f"  å¹³å‡ESS (bulk): {mean_ess:.1f}")
        
        result['n_params'] = n_params
        result['mean_rhat'] = float(mean_rhat) if not np.isnan(mean_rhat) else None
        result['mean_ess'] = float(mean_ess) if not np.isnan(mean_ess) else None
        
    except Exception as e:
        print(f"  âš ï¸ ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. WAICè¨ˆç®—ã‚’è©¦ã¿ã‚‹ï¼ˆlog_likelihoodãŒå¿…è¦ï¼‰
    try:
        if 'log_likelihood' in trace:
            waic = az.waic(trace, pointwise=True)
            print(f"\nğŸ“Š {model_name} WAICçµ±è¨ˆ:")
            print(f"  ELPD WAIC: {waic.elpd_waic:.2f} Â± {waic.se:.2f}")
            print(f"  p_waic (æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°): {waic.p_waic:.2f}")
            
            result['elpd_waic'] = float(waic.elpd_waic)
            result['se_waic'] = float(waic.se)
            result['p_waic'] = float(waic.p_waic)
            result['has_waic'] = True
        else:
            print(f"\n  âš ï¸ log_likelihoodãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„ãŸã‚WAICè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            print(f"     (SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã¯log_likelihoodãŒè‡ªå‹•ä¿å­˜ã•ã‚Œã¾ã›ã‚“)")
            result['has_waic'] = False
            
    except Exception as e:
        print(f"  âš ï¸ WAICè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        result['has_waic'] = False
    
    # 3. äº‹å¾Œäºˆæ¸¬ã®åŸºæœ¬çµ±è¨ˆ
    try:
        posterior = trace.posterior
        n_chains = posterior.dims.get('chain', 1)
        n_draws = posterior.dims.get('draw', 0)
        total_samples = n_chains * n_draws
        
        print(f"\nğŸ“Š {model_name} äº‹å¾Œåˆ†å¸ƒçµ±è¨ˆ:")
        print(f"  ãƒã‚§ãƒ¼ãƒ³æ•°: {n_chains}")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°/ãƒã‚§ãƒ¼ãƒ³: {n_draws}")
        print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
        
        result['n_chains'] = n_chains
        result['n_draws'] = n_draws
        result['total_samples'] = total_samples
        
    except Exception as e:
        print(f"  âš ï¸ äº‹å¾Œåˆ†å¸ƒçµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
    
    return result


def compare_models(eval_H, eval_B):
    """Hå½¢å¼ã¨Bå½¢å¼ã®ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
    print(f"\n{'='*80}")
    print(f"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print(f"{'='*80}")
    
    try:
        # WAICæ¯”è¼ƒï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if eval_H.get('has_waic') and eval_B.get('has_waic'):
            elpd_H = eval_H['elpd_waic']
            elpd_B = eval_B['elpd_waic']
            se_H = eval_H['se_waic']
            se_B = eval_B['se_waic']
            
            elpd_diff = elpd_H - elpd_B
            se_diff = np.sqrt(se_H**2 + se_B**2)
            
            print(f"\nğŸ“Š ELPD WAICå·®åˆ† (H-form - B-form):")
            print(f"  Î”ELPD: {elpd_diff:.2f} Â± {se_diff:.2f}")
            
            if abs(elpd_diff) < 2 * se_diff:
                print(f"  â†’ ãƒ¢ãƒ‡ãƒ«é–“ã«æœ‰æ„ãªå·®ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆ|Î”ELPD| < 2Ã—SEï¼‰")
                winner = "å¼•ãåˆ†ã‘"
            elif elpd_diff > 0:
                print(f"  â†’ Hå½¢å¼ãŒå„ªã‚Œã¦ã„ã¾ã™ï¼ˆÎ”ELPD > 2Ã—SEï¼‰")
                winner = "H-form"
            else:
                print(f"  â†’ Bå½¢å¼ãŒå„ªã‚Œã¦ã„ã¾ã™ï¼ˆÎ”ELPD < -2Ã—SEï¼‰")
                winner = "B-form"
            
            # æ¯”è¼ƒã‚µãƒãƒªãƒ¼
            print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼:")
            print(f"  {'ãƒ¢ãƒ‡ãƒ«':<10} {'ELPD WAIC':<15} {'SE':<10} {'p_waic':<10}")
            print(f"  {'-'*45}")
            print(f"  {'H-form':<10} {elpd_H:<15.2f} {se_H:<10.2f} {eval_H['p_waic']:<10.2f}")
            print(f"  {'B-form':<10} {elpd_B:<15.2f} {se_B:<10.2f} {eval_B['p_waic']:<10.2f}")
            print(f"\n  ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {winner}")
            
            return {
                'elpd_diff': elpd_diff,
                'se_diff': se_diff,
                'winner': winner,
                'method': 'WAIC'
            }
        else:
            # WAICãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ESSæ¯”è¼ƒ
            print(f"\n  âš ï¸ WAICãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å“è³ªã§æ¯”è¼ƒ")
            
            ess_H = eval_H.get('mean_ess', 0)
            ess_B = eval_B.get('mean_ess', 0)
            
            print(f"\nğŸ“Š ESS (Effective Sample Size) æ¯”è¼ƒ:")
            print(f"  H-form ESS: {ess_H:.1f}")
            print(f"  B-form ESS: {ess_B:.1f}")
            
            if ess_H > ess_B * 1.1:
                winner = "H-form (ã‚ˆã‚Šè‰¯ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)"
            elif ess_B > ess_H * 1.1:
                winner = "B-form (ã‚ˆã‚Šè‰¯ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)"
            else:
                winner = "å¼•ãåˆ†ã‘"
            
            print(f"\n  ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {winner}")
            
            return {
                'ess_H': ess_H,
                'ess_B': ess_B,
                'winner': winner,
                'method': 'ESS comparison'
            }
    
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
        return None


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
def compute_loo_cv(trace, model_name='Model'):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆcompute_model_evaluationã‚’å‘¼ã³å‡ºã™ï¼‰"""
    return compute_model_evaluation(trace, model_name)


def compare_models_loo(eval_H, eval_B):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆcompare_modelsã‚’å‘¼ã³å‡ºã™ï¼‰"""
    return compare_models(eval_H, eval_B)


# ============================================================================
# ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—ï¼ˆSMCå¯¾å¿œï¼‰
# ============================================================================
def compute_bayes_factor_smc(trace_H, trace_B):
    """
    SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã‹ã‚‰ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æ¨å®š
    
    SMCã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯å‘¨è¾ºå°¤åº¦ï¼ˆmarginal likelihoodï¼‰ã®æ¨å®šå€¤ã‚’
    sample_stats.log_marginal_likelihoodã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
    
    ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: BF_{H/B} = P(D|M_H) / P(D|M_B)
    å¯¾æ•°ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: log(BF) = log(P(D|M_H)) - log(P(D|M_B))
    
    Jeffreys (1961) ã®è§£é‡ˆåŸºæº–:
    |logâ‚â‚€(BF)|  |ln(BF)|   å¼·ã•
    0 - 0.5      0 - 1.15   ã»ã¼è¨¼æ‹ ãªã—
    0.5 - 1      1.15 - 2.3 å¼±ã„è¨¼æ‹ 
    1 - 2        2.3 - 4.6  ä¸­ç¨‹åº¦ã®è¨¼æ‹ 
    > 2          > 4.6      å¼·ã„è¨¼æ‹ 
    
    Returns:
        dict: ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–¢é€£ã®çµ±è¨ˆé‡
    """
    print(f"\n{'='*80}")
    print("ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—ï¼ˆSMCå‘¨è¾ºå°¤åº¦ãƒ™ãƒ¼ã‚¹ï¼‰")
    print(f"{'='*80}")
    
    result = {}
    
    try:
        # SMCã®sample_statsã‹ã‚‰å‘¨è¾ºå°¤åº¦ã‚’å–å¾—
        has_lml_H = hasattr(trace_H, 'sample_stats') and 'log_marginal_likelihood' in trace_H.sample_stats
        has_lml_B = hasattr(trace_B, 'sample_stats') and 'log_marginal_likelihood' in trace_B.sample_stats
        
        if not has_lml_H or not has_lml_B:
            print("  âš ï¸ SMCå‘¨è¾ºå°¤åº¦ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("     PyMC >= 5.0 ã® sample_smc() ã§è¨ˆç®—ã•ã‚Œã¾ã™")
            
            # ä»£æ›¿: WAICå·®åˆ†ã‹ã‚‰ã®è¿‘ä¼¼BFï¼ˆBridge Samplingã®ä»£æ›¿ï¼‰
            print("\n  â†’ WAICãƒ™ãƒ¼ã‚¹ã®è¿‘ä¼¼ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—...")
            return _compute_approximate_bf_from_waic(trace_H, trace_B)
        
        # å‘¨è¾ºå°¤åº¦ã®å–å¾—ï¼ˆå…¨ãƒã‚§ãƒ¼ãƒ³ã®å¹³å‡ï¼‰
        lml_H = float(trace_H.sample_stats['log_marginal_likelihood'].values.mean())
        lml_B = float(trace_B.sample_stats['log_marginal_likelihood'].values.mean())
        
        # ãƒã‚§ãƒ¼ãƒ³é–“ã®æ¨™æº–åå·®ï¼ˆä¸ç¢ºå®Ÿæ€§æ¨å®šï¼‰
        lml_H_std = float(trace_H.sample_stats['log_marginal_likelihood'].values.std())
        lml_B_std = float(trace_B.sample_stats['log_marginal_likelihood'].values.std())
        
        # å¯¾æ•°ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆHå½¢å¼ vs Bå½¢å¼ï¼‰
        log_BF = lml_H - lml_B
        log_BF_se = np.sqrt(lml_H_std**2 + lml_B_std**2)
        
        # log10ã‚¹ã‚±ãƒ¼ãƒ«ã¸ã®å¤‰æ›
        log10_BF = log_BF / np.log(10)
        
        print(f"\nğŸ“Š å‘¨è¾ºå°¤åº¦ (log scale):")
        print(f"  Hå½¢å¼: {lml_H:.2f} Â± {lml_H_std:.2f}")
        print(f"  Bå½¢å¼: {lml_B:.2f} Â± {lml_B_std:.2f}")
        print(f"\nğŸ“Š ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼:")
        print(f"  log(BF_{{H/B}}): {log_BF:.2f} Â± {log_BF_se:.2f}")
        print(f"  logâ‚â‚€(BF_{{H/B}}): {log10_BF:.2f}")
        
        # Jeffreysã®è§£é‡ˆåŸºæº–
        abs_log_BF = abs(log_BF)
        if abs_log_BF < 1.15:  # |log10| < 0.5
            strength = "ã»ã¼è¨¼æ‹ ãªã— (Barely worth mentioning)"
        elif abs_log_BF < 2.3:  # |log10| < 1
            strength = "å¼±ã„è¨¼æ‹  (Substantial)"
        elif abs_log_BF < 4.6:  # |log10| < 2
            strength = "ä¸­ç¨‹åº¦ã®è¨¼æ‹  (Strong)"
        else:
            strength = "å¼·ã„è¨¼æ‹  (Decisive)"
        
        if log_BF > 0:
            winner = "H-form"
            favor = "Hå½¢å¼ã‚’æ”¯æŒ"
        elif log_BF < 0:
            winner = "B-form"
            favor = "Bå½¢å¼ã‚’æ”¯æŒ"
        else:
            winner = "å¼•ãåˆ†ã‘"
            favor = "ã©ã¡ã‚‰ã‚‚åŒç­‰"
        
        print(f"\nğŸ“Š Jeffreys (1961) è§£é‡ˆ:")
        print(f"  {favor}: {strength}")
        print(f"\n  ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {winner}")
        
        result = {
            'log_marginal_likelihood_H': lml_H,
            'log_marginal_likelihood_B': lml_B,
            'log_marginal_likelihood_H_std': lml_H_std,
            'log_marginal_likelihood_B_std': lml_B_std,
            'log_BF': log_BF,
            'log_BF_se': log_BF_se,
            'log10_BF': log10_BF,
            'interpretation': strength,
            'winner': winner,
            'method': 'SMC marginal likelihood'
        }
        
    except Exception as e:
        print(f"âŒ ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        result = {'error': str(e), 'winner': 'N/A'}
    
    return result


def _compute_approximate_bf_from_waic(trace_H, trace_B):
    """
    WAICãƒ™ãƒ¼ã‚¹ã®è¿‘ä¼¼ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—
    
    WAICï¼ˆåºƒãé©ç”¨å¯èƒ½ãªæƒ…å ±é‡è¦æº–ï¼‰ã‹ã‚‰ELPDã‚’ç”¨ã„ã¦
    ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¿‘ä¼¼ã™ã‚‹æ–¹æ³•ã€‚
    
    æ³¨æ„: ã“ã‚Œã¯å³å¯†ãªãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã§ã¯ãªãã€
    äºˆæ¸¬æ€§èƒ½ã«åŸºã¥ãè¿‘ä¼¼å€¤ã§ã™ã€‚
    """
    result = {'method': 'WAIC approximation (fallback)'}
    
    try:
        # WAICè¨ˆç®—
        if 'log_likelihood' not in trace_H or 'log_likelihood' not in trace_B:
            print("  âš ï¸ log_likelihoodãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„ãŸã‚è¨ˆç®—ä¸å¯")
            result['error'] = 'log_likelihood not available'
            result['winner'] = 'N/A'
            return result
        
        waic_H = az.waic(trace_H)
        waic_B = az.waic(trace_B)
        
        # ELPDå·®åˆ†ã‹ã‚‰ã®è¿‘ä¼¼log(BF)
        # ELPD â‰ˆ log(predictive performance) ãªã®ã§
        # Î”ELPD â‰ˆ log(BF) ã®è¿‘ä¼¼ã¨ã—ã¦ä½¿ç”¨
        elpd_diff = waic_H.elpd_waic - waic_B.elpd_waic
        se_diff = np.sqrt(waic_H.se**2 + waic_B.se**2)
        
        print(f"\nğŸ“Š WAICè¿‘ä¼¼ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼:")
        print(f"  ELPD Hå½¢å¼: {waic_H.elpd_waic:.2f} Â± {waic_H.se:.2f}")
        print(f"  ELPD Bå½¢å¼: {waic_B.elpd_waic:.2f} Â± {waic_B.se:.2f}")
        print(f"  Î”ELPD (â‰ˆlog BF): {elpd_diff:.2f} Â± {se_diff:.2f}")
        
        # æœ‰æ„æ€§åˆ¤å®š
        if abs(elpd_diff) < 2 * se_diff:
            winner = "å¼•ãåˆ†ã‘"
            interpretation = "æœ‰æ„ãªå·®ãªã—"
        elif elpd_diff > 0:
            winner = "H-form"
            interpretation = "Hå½¢å¼ãŒå„ªã‚ŒãŸäºˆæ¸¬æ€§èƒ½"
        else:
            winner = "B-form"
            interpretation = "Bå½¢å¼ãŒå„ªã‚ŒãŸäºˆæ¸¬æ€§èƒ½"
        
        print(f"\n  {interpretation}")
        print(f"  ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {winner}")
        
        result.update({
            'elpd_H': float(waic_H.elpd_waic),
            'elpd_B': float(waic_B.elpd_waic),
            'elpd_diff': float(elpd_diff),
            'se_diff': float(se_diff),
            'log_BF': float(elpd_diff),  # è¿‘ä¼¼å€¤ã¨ã—ã¦
            'interpretation': interpretation,
            'winner': winner
        })
        
    except Exception as e:
        print(f"  âš ï¸ WAICè¿‘ä¼¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        result['error'] = str(e)
        result['winner'] = 'N/A'
    
    return result


# ============================================================================
# ãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°ï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’æµç”¨ã€calculate_transmission_for_paramsã®ã¿è¿½åŠ ï¼‰
# ============================================================================
def calculate_transmission_for_params(freq, B, T, g, a, B4, B6, eps, gamma_array, model_form='H'):
    """æŒ‡å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—"""
    H_ham = get_hamiltonian(B, g, B4, B6)
    chi_raw = calculate_susceptibility(freq, H_ham, T, gamma_array)
    G0 = a * mu0 * N_SPIN * (g * muB)**2 / (2 * hbar) / THZ_TO_RAD_S
    chi = G0 * chi_raw
    
    if model_form == 'H':
        mu_r = 1.0 + chi
    else:
        denominator = 1.0 - chi
        mu_r = 1.0 / denominator
    
    trans = calculate_transmission(freq, mu_r, d_fixed, eps)
    return trans


def plot_prior_distributions(v6_params, model_form='H', save_dir=None):
    """
    äº‹å‰åˆ†å¸ƒã®å¯è¦–åŒ–ï¼ˆç‰©ç†å€¤ç©ºé–“ï¼‰- v7éšå±¤ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
    
    v7ã®æ–°ã—ã„äº‹å‰åˆ†å¸ƒ:
    - g: TruncatedNormal(Î¼=2.0, Ïƒ=0.05)
    - a: HalfNormal(Ïƒ=2.0) + clip[0.1, 10]
    - Bâ‚„: LogNormal(Î¼=log(2mK), Ïƒ=1.2) + clip[0.01mK, 50mK]
    - Bâ‚†: Normal(Î¼=0, Ïƒ=0.5mK) + clip[-2mK, 2mK]
    - Îµ_bg: TruncatedNormal(Î¼=v6å¹³å‡, Ïƒ=0.3)
    - Î³_mean: TruncatedNormal(Î¼=0.07, Ïƒ=0.04)
    - Î³_std: HalfNormal(Ïƒ=0.03)
    - Î³_i: TruncatedNormal(Î¼=Î³_mean, Ïƒ=Î³_std) (éšå±¤ãƒ¢ãƒ‡ãƒ«)
    """
    from scipy.stats import halfnorm, lognorm, norm
    
    print(f"\n{'='*80}")
    print(f"äº‹å‰åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆ ({model_form}-form) [v7éšå±¤ãƒ¢ãƒ‡ãƒ«]")
    print(f"{'='*80}")
    
    fig, axes = plt.subplots(3, 5, figsize=(20, 13))
    fig.suptitle(f'Prior Distributions ({model_form}-form) - v7 Hierarchical', fontsize=16, y=0.98)
    axes = axes.flatten()
    
    # 1. g_factor: TruncatedNormal(Î¼=2.0, Ïƒ=0.05, [1.5, 2.8])
    ax = axes[0]
    g_range = np.linspace(1.5, 2.8, 500)
    a_trunc, b_trunc = (1.5 - 2.0) / 0.05, (2.8 - 2.0) / 0.05
    g_prior = truncnorm.pdf(g_range, a_trunc, b_trunc, loc=2.0, scale=0.05)
    ax.plot(g_range, g_prior, 'b-', lw=2, label='Prior')
    ax.axvline(v6_params['g'], color='r', linestyle='--', lw=1.5, label=f'v6: {v6_params["g"]:.2f}')
    ax.axvline(2.0, color='g', linestyle=':', lw=1.5, label='Theory: 2.0')
    ax.set_xlabel('g-factor', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('g-factor (TruncNormal)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    
    # 2. a_scale: HalfNormal(Ïƒ=2.0) + clip[0.1, 10]
    ax = axes[1]
    a_range = np.linspace(0.0, 12, 500)
    a_prior_raw = halfnorm.pdf(a_range, scale=2.0)
    # clipåŠ¹æœã‚’è¿‘ä¼¼çš„ã«è¡¨ç¤º
    a_prior = np.where((a_range >= 0.1) & (a_range <= 10.0), a_prior_raw, 0.0)
    ax.plot(a_range, a_prior, 'b-', lw=2, label='Prior (HalfNormal)')
    ax.axvline(v6_params['a'], color='r', linestyle='--', lw=1.5, label=f'v6: {v6_params["a"]:.2f}')
    ax.axvspan(0, 0.1, alpha=0.2, color='gray', label='Clipped')
    ax.axvspan(10, 12, alpha=0.2, color='gray')
    ax.set_xlabel('a', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('a (HalfNormal Ïƒ=2)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    ax.set_xlim([0, 12])
    
    # 3. B4: LogNormal(Î¼=log(2mK), Ïƒ=1.2) + clip[0.01mK, 50mK]
    ax = axes[2]
    B4_range = np.linspace(0.001, 60, 500)  # mKå˜ä½
    B4_log_mu = np.log(2.0)  # 2mK
    B4_log_sigma = 1.2
    B4_prior_raw = lognorm.pdf(B4_range, s=B4_log_sigma, scale=np.exp(B4_log_mu))
    B4_prior = np.where((B4_range >= 0.01) & (B4_range <= 50.0), B4_prior_raw, 0.0)
    ax.plot(B4_range, B4_prior, 'b-', lw=2, label='Prior (LogNormal)')
    ax.axvline(v6_params['B4'] * 1000, color='r', linestyle='--', lw=1.5, 
               label=f'v6: {v6_params["B4"]*1000:.1f}mK')
    ax.axvspan(50, 60, alpha=0.2, color='gray', label='Clipped')
    ax.set_xlabel('Bâ‚„ (mK)', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('Bâ‚„ (LogNormal Î¼=2mK)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    ax.set_xlim([0, 60])
    
    # 4. B6: Normal(Î¼=0, Ïƒ=0.5mK) + clip[-2mK, 2mK]
    ax = axes[3]
    B6_range = np.linspace(-2.5, 2.5, 500)  # mKå˜ä½
    B6_prior_raw = norm.pdf(B6_range, loc=0, scale=0.5)
    B6_prior = np.where((B6_range >= -2.0) & (B6_range <= 2.0), B6_prior_raw, 0.0)
    ax.plot(B6_range, B6_prior, 'b-', lw=2, label='Prior (Normal)')
    ax.axvline(v6_params['B6'] * 1000, color='r', linestyle='--', lw=1.5, 
               label=f'v6: {v6_params["B6"]*1000:.2f}mK')
    ax.axvspan(-2.5, -2.0, alpha=0.2, color='gray', label='Clipped')
    ax.axvspan(2.0, 2.5, alpha=0.2, color='gray')
    ax.set_xlabel('Bâ‚† (mK)', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('Bâ‚† (Normal Ïƒ=0.5mK)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    
    # 5. eps_bg: TruncatedNormal(Î¼=v6å¹³å‡, Ïƒ=0.3, [13, 16])
    ax = axes[4]
    eps_range = np.linspace(12, 17, 500)
    eps_mu = v6_params['eps']  # v6å¹³å‡å€¤ã‚’ä¸­å¿ƒ
    eps_sigma = 0.3  # v7ã§0.5â†’0.3ã«å¤‰æ›´
    a_trunc_eps = (13.0 - eps_mu) / eps_sigma
    b_trunc_eps = (16.0 - eps_mu) / eps_sigma
    eps_prior = truncnorm.pdf(eps_range, a_trunc_eps, b_trunc_eps, loc=eps_mu, scale=eps_sigma)
    ax.plot(eps_range, eps_prior, 'b-', lw=2, label='Prior')
    ax.axvline(eps_mu, color='r', linestyle='--', lw=1.5, label=f'v6: {eps_mu:.1f}')
    ax.set_xlabel('Îµ_bg', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('Îµ_bg (TruncNormal Ïƒ=0.3)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    
    # 6. Î³_mean (éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿): TruncatedNormal(Î¼=0.074, Ïƒ=0.16, [0.005, 0.3])
    ax = axes[5]
    gamma_mean_range = np.linspace(0, 0.35, 500)
    gamma_mean_mu = GAMMA_HYPERPRIOR_MU  # 0.074
    gamma_mean_sigma = GAMMA_HYPERPRIOR_SIGMA  # 0.16
    a_trunc_gm = (0.005 - gamma_mean_mu) / gamma_mean_sigma
    b_trunc_gm = (0.3 - gamma_mean_mu) / gamma_mean_sigma
    gamma_mean_prior = truncnorm.pdf(gamma_mean_range, a_trunc_gm, b_trunc_gm, 
                                      loc=gamma_mean_mu, scale=gamma_mean_sigma)
    ax.plot(gamma_mean_range, gamma_mean_prior, 'b-', lw=2, label='Prior')
    # v6ã®éå¼µã‚Šä»˜ãÎ³å¹³å‡
    v6_gamma_nonbound = [g for g in v6_params['gamma'] if g > 0.015]
    if v6_gamma_nonbound:
        v6_gamma_mean = np.mean(v6_gamma_nonbound)
        ax.axvline(v6_gamma_mean, color='r', linestyle='--', lw=1.5, 
                   label=f'v6 mean: {v6_gamma_mean*1000:.0f}GHz')
    ax.set_xlabel('Î³_mean (THz)', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('Î³_mean (Hierarchical)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    
    # 7. Î³_std (éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿): HalfNormal(Ïƒ=0.092)
    ax = axes[6]
    gamma_std_range = np.linspace(0, 0.15, 500)
    gamma_std_prior = halfnorm.pdf(gamma_std_range, scale=GAMMA_STD_PRIOR)  # 0.092
    ax.plot(gamma_std_range, gamma_std_prior, 'b-', lw=2, label='Prior (HalfNormal)')
    # v6ã®Î³æ¨™æº–åå·®
    if v6_gamma_nonbound:
        v6_gamma_std = np.std(v6_gamma_nonbound)
        ax.axvline(v6_gamma_std, color='r', linestyle='--', lw=1.5, 
                   label=f'v6 std: {v6_gamma_std*1000:.0f}GHz')
    ax.set_xlabel('Î³_std (THz)', fontsize=9)
    ax.set_ylabel('Prob. Density', fontsize=9)
    ax.set_title('Î³_std (Hierarchical)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right')
    ax.grid(alpha=0.3)
    
    # 8-14. gamma_1 ~ gamma_7: éšå±¤ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥Î³
    # éšå±¤ãƒ¢ãƒ‡ãƒ«ã§ã¯Î³_meanã¨Î³_stdã«å¾“ã†ãŸã‚ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’è¡¨ç¤º
    gamma_range = np.linspace(0, 0.6, 500)
    # Î³_mean=0.07, Î³_std=0.03ã®å ´åˆã®å…¸å‹çš„ãªåˆ†å¸ƒ
    a_trunc_gamma = (0.005 - gamma_mean_mu) / (GAMMA_STD_PRIOR + 1e-6)
    b_trunc_gamma = (0.5 - gamma_mean_mu) / (GAMMA_STD_PRIOR + 1e-6)
    gamma_prior = truncnorm.pdf(gamma_range, a_trunc_gamma, b_trunc_gamma, 
                                 loc=gamma_mean_mu, scale=GAMMA_STD_PRIOR)
    
    for i in range(7):
        ax = axes[7 + i]
        ax.plot(gamma_range, gamma_prior, 'b-', lw=2, label='Prior (Hierarchical)')
        if i < len(v6_params['gamma']):
            ax.axvline(v6_params['gamma'][i], color='r', linestyle='--', lw=1.5, 
                      label=f'v6: {v6_params["gamma"][i]*1000:.0f}GHz')
        ax.set_xlabel(f'Î³_{i+1} (THz)', fontsize=9)
        ax.set_ylabel('Prob. Density', fontsize=9)
        ax.set_title(f'Î³_{i+1} (Pooled)', fontsize=10, fontweight='bold')
        ax.legend(fontsize=7, loc='upper right')
        ax.grid(alpha=0.3)
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(14, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    
    if save_dir:
        save_path = save_dir / f'prior_distributions_{model_form}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ prior_distributions_{model_form}.png saved")
    
    plt.close()


def plot_posterior_distributions(trace, model_form='H', save_dir=None):
    """äº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–ï¼ˆArviZã‚’ä½¿ç”¨ï¼‰"""
    print(f"\n{'='*80}")
    print(f"äº‹å¾Œåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆ ({model_form}-form)")
    print(f"{'='*80}")
    
    # ç‰©ç†å€¤ã¸ã®å¤‰æ›
    posterior = trace.posterior
    
    var_names_scaled = ['g_factor_scaled', 'a_scale_scaled', 'B4_scaled', 'B6_scaled', 'eps_bg_scaled']
    var_names_scaled += [f'gamma_{i+1}_scaled' for i in range(7)]
    
    # 1. ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåæŸè¨ºæ–­ï¼‰
    fig = az.plot_trace(trace, var_names=var_names_scaled[:5], compact=True, figsize=(15, 12))
    fig[0, 0].figure.suptitle(f'Trace Plot - Global Parameters ({model_form}-form)', fontsize=14, y=0.995)
    if save_dir:
        plt.savefig(save_dir / f'trace_global_{model_form}.png', dpi=300, bbox_inches='tight')
        print(f"  âœ“ trace_global_{model_form}.png saved")
    plt.close()
    
    fig = az.plot_trace(trace, var_names=var_names_scaled[5:], compact=True, figsize=(15, 18))
    fig[0, 0].figure.suptitle(f'Trace Plot - Gamma Parameters ({model_form}-form)', fontsize=14, y=0.995)
    if save_dir:
        plt.savefig(save_dir / f'trace_gamma_{model_form}.png', dpi=300, bbox_inches='tight')
        print(f"  âœ“ trace_gamma_{model_form}.png saved")
    plt.close()
    
    # 2. äº‹å¾Œåˆ†å¸ƒï¼ˆç‰©ç†å€¤ç©ºé–“ã§ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    fig.suptitle(f'Posterior Distributions ({model_form}-form)', fontsize=16, y=0.995)
    axes = axes.flatten()
    
    # ç‰©ç†å€¤ã¸ã®å¤‰æ›ã¨ãƒ—ãƒ­ãƒƒãƒˆ
    param_info = [
        ('g_factor_scaled', 'g', SCALING_FACTORS['g'], 'g-factor', ''),
        ('a_scale_scaled', 'a', SCALING_FACTORS['a'], 'a (coupling)', ''),
        ('B4_scaled', 'B4', SCALING_FACTORS['B4'], 'Bâ‚„', 'mK'),
        ('B6_scaled', 'B6', SCALING_FACTORS['B6'], 'Bâ‚†', 'mK'),
        ('eps_bg_scaled', 'eps', SCALING_FACTORS['eps'], 'Îµ_bg', ''),
    ]
    
    for i, (var_scaled, var_phys, scale_factor, label, unit) in enumerate(param_info):
        ax = axes[i]
        samples_scaled = posterior[var_scaled].values.flatten()
        samples_phys = samples_scaled / scale_factor
        
        # mKå˜ä½ã«å¤‰æ›
        if unit == 'mK':
            samples_phys = samples_phys * 1000
            xlabel = f'{label} ({unit})'
        else:
            xlabel = label
        
        ax.hist(samples_phys, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black', linewidth=0.5)
        
        mean_val = np.mean(samples_phys)
        median_val = np.median(samples_phys)
        hdi = az.hdi(samples_phys, hdi_prob=0.94)
        
        ax.axvline(mean_val, color='red', linestyle='--', lw=1.5, label=f'Mean: {mean_val:.3g}')
        ax.axvline(median_val, color='orange', linestyle='-.', lw=1.5, label=f'Med: {median_val:.3g}')
        ax.axvspan(hdi[0], hdi[1], alpha=0.2, color='green', label=f'94% HDI')
        
        ax.set_xlabel(xlabel, fontsize=9)
        ax.set_ylabel('Density', fontsize=9)
        ax.set_title(f'{label}', fontsize=10, fontweight='bold')
        ax.legend(fontsize=6, loc='upper right', framealpha=0.8)
        ax.grid(alpha=0.3)
        ax.tick_params(axis='both', labelsize=8)
    
    # gamma parameters
    for i in range(7):
        ax = axes[5 + i]
        var_name = f'gamma_{i+1}_scaled'
        samples_scaled = posterior[var_name].values.flatten()
        samples_phys = samples_scaled / SCALING_FACTORS['gamma']
        
        ax.hist(samples_phys, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black', linewidth=0.5)
        
        mean_val = np.mean(samples_phys)
        median_val = np.median(samples_phys)
        hdi = az.hdi(samples_phys, hdi_prob=0.94)
        
        ax.axvline(mean_val, color='red', linestyle='--', lw=1.5, label=f'Mean: {mean_val:.2f}')
        ax.axvline(median_val, color='orange', linestyle='-.', lw=1.5, label=f'Med: {median_val:.2f}')
        ax.axvspan(hdi[0], hdi[1], alpha=0.2, color='green', label=f'94% HDI')
        
        ax.set_xlabel(f'Î³_{i+1} (THz)', fontsize=9)
        ax.set_ylabel('Density', fontsize=9)
        ax.set_title(f'Î³_{i+1}', fontsize=10, fontweight='bold')
        ax.legend(fontsize=6, loc='upper right', framealpha=0.8)
        ax.grid(alpha=0.3)
        ax.tick_params(axis='both', labelsize=8)
    
    plt.tight_layout()
    
    if save_dir:
        save_path = save_dir / f'posterior_distributions_{model_form}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ posterior_distributions_{model_form}.png saved")
    
    plt.close()
    
    # 3. ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç›¸é–¢ï¼‰
    print("  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    pair_vars = ['g_factor_scaled', 'a_scale_scaled', 'B4_scaled', 'B6_scaled', 'eps_bg_scaled']
    axes_pair = az.plot_pair(trace, var_names=pair_vars, kind='kde', 
                             marginals=True, figsize=(14, 14))
    # az.plot_pairã¯axesã®é…åˆ—ã‚’è¿”ã™ã®ã§ã€figureã‚’å–å¾—
    if hasattr(axes_pair, 'flatten'):
        fig_pair = axes_pair.flatten()[0].figure
    else:
        fig_pair = axes_pair[0, 0].figure
    fig_pair.suptitle(f'Parameter Correlations ({model_form}-form)', fontsize=14, y=0.995)
    if save_dir:
        plt.savefig(save_dir / f'pair_plot_{model_form}.png', dpi=300, bbox_inches='tight')
        print(f"  âœ“ pair_plot_{model_form}.png saved")
    plt.close()


def plot_prior_posterior_comparison(trace, v6_params, model_form='H', save_dir=None):
    """
    äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ - v7éšå±¤ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
    
    æ–°ã—ã„äº‹å‰åˆ†å¸ƒã‚’åæ˜ :
    - a: HalfNormal(Ïƒ=2)
    - Bâ‚„: LogNormal(Î¼=log(2mK), Ïƒ=1.2)
    - Bâ‚†: Normal(Î¼=0, Ïƒ=0.5mK)
    - Î³: éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ³_mean, Î³_stdã‹ã‚‰ç”Ÿæˆï¼‰
    """
    from scipy.stats import halfnorm, lognorm, norm
    
    print(f"\n{'='*80}")
    print(f"äº‹å‰åˆ†å¸ƒ vs äº‹å¾Œåˆ†å¸ƒ æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆ ({model_form}-form) [v7]")
    print(f"{'='*80}")
    
    posterior = trace.posterior
    
    fig, axes = plt.subplots(3, 5, figsize=(20, 13))
    fig.suptitle(f'Prior vs Posterior Comparison ({model_form}-form) - v7 Hierarchical', fontsize=16, y=0.98)
    axes = axes.flatten()
    
    # 1. g_factor: TruncatedNormal(Î¼=2.0, Ïƒ=0.05)
    ax = axes[0]
    g_range = np.linspace(1.5, 2.8, 500)
    a_trunc, b_trunc = (1.5 - 2.0) / 0.05, (2.8 - 2.0) / 0.05
    g_prior = truncnorm.pdf(g_range, a_trunc, b_trunc, loc=2.0, scale=0.05)
    
    samples_g = posterior['g_factor_scaled'].values.flatten() / SCALING_FACTORS['g']
    ax.hist(samples_g, bins=50, density=True, alpha=0.6, color='steelblue', 
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(g_range, g_prior, 'r-', lw=2, label='Prior')
    ax.axvline(v6_params['g'], color='orange', linestyle='--', lw=1.5, label=f'v6: {v6_params["g"]:.2f}')
    ax.set_xlabel('g-factor', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('g-factor (TruncNormal)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 2. a_scale: HalfNormal(Ïƒ=2.0) + clip[0.1, 10]
    ax = axes[1]
    a_range = np.linspace(0.0, 12, 500)
    a_prior_raw = halfnorm.pdf(a_range, scale=2.0)
    a_prior = np.where((a_range >= 0.1) & (a_range <= 10.0), a_prior_raw, 0.0)
    
    samples_a = posterior['a_scale_scaled'].values.flatten() / SCALING_FACTORS['a']
    ax.hist(samples_a, bins=50, density=True, alpha=0.6, color='steelblue', 
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(a_range, a_prior, 'r-', lw=2, label='Prior')
    ax.axvline(v6_params['a'], color='orange', linestyle='--', lw=1.5, label=f'v6: {v6_params["a"]:.2f}')
    ax.set_xlabel('a', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('a (HalfNormal)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 3. B4: LogNormal(Î¼=log(2mK), Ïƒ=1.2) + clip[0.01mK, 50mK]
    ax = axes[2]
    B4_range = np.linspace(0.001, 60, 500)  # mK
    B4_log_mu = np.log(2.0)
    B4_log_sigma = 1.2
    B4_prior_raw = lognorm.pdf(B4_range, s=B4_log_sigma, scale=np.exp(B4_log_mu))
    B4_prior = np.where((B4_range >= 0.01) & (B4_range <= 50.0), B4_prior_raw, 0.0)
    
    samples_B4 = posterior['B4_scaled'].values.flatten() / SCALING_FACTORS['B4'] * 1000  # mK
    ax.hist(samples_B4, bins=50, density=True, alpha=0.6, color='steelblue', 
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(B4_range, B4_prior, 'r-', lw=2, label='Prior')
    ax.axvline(v6_params['B4'] * 1000, color='orange', linestyle='--', lw=1.5, 
               label=f'v6: {v6_params["B4"]*1000:.1f}mK')
    ax.set_xlabel('Bâ‚„ (mK)', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('Bâ‚„ (LogNormal)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 4. B6: Normal(Î¼=0, Ïƒ=0.5mK) + clip[-2mK, 2mK]
    ax = axes[3]
    B6_range = np.linspace(-2.5, 2.5, 500)  # mK
    B6_prior_raw = norm.pdf(B6_range, loc=0, scale=0.5)
    B6_prior = np.where((B6_range >= -2.0) & (B6_range <= 2.0), B6_prior_raw, 0.0)
    
    samples_B6 = posterior['B6_scaled'].values.flatten() / SCALING_FACTORS['B6'] * 1000  # mK
    ax.hist(samples_B6, bins=50, density=True, alpha=0.6, color='steelblue', 
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(B6_range, B6_prior, 'r-', lw=2, label='Prior')
    ax.axvline(v6_params['B6'] * 1000, color='orange', linestyle='--', lw=1.5, 
               label=f'v6: {v6_params["B6"]*1000:.2f}mK')
    ax.set_xlabel('Bâ‚† (mK)', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('Bâ‚† (Normal)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 5. eps_bg: TruncatedNormal(Î¼=v6å¹³å‡, Ïƒ=0.3)
    ax = axes[4]
    eps_range = np.linspace(12, 17, 500)
    eps_mu = v6_params['eps']
    eps_sigma = 0.3  # v7: 0.5â†’0.3
    a_trunc_eps = (13.0 - eps_mu) / eps_sigma
    b_trunc_eps = (16.0 - eps_mu) / eps_sigma
    eps_prior = truncnorm.pdf(eps_range, a_trunc_eps, b_trunc_eps, loc=eps_mu, scale=eps_sigma)
    
    samples_eps = posterior['eps_bg_scaled'].values.flatten() / SCALING_FACTORS['eps']
    ax.hist(samples_eps, bins=50, density=True, alpha=0.6, color='steelblue', 
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(eps_range, eps_prior, 'r-', lw=2, label='Prior')
    ax.axvline(eps_mu, color='orange', linestyle='--', lw=1.5, label=f'v6: {eps_mu:.1f}')
    ax.set_xlabel('Îµ_bg', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('Îµ_bg (TruncNormal Ïƒ=0.3)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 6. Î³_mean (éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
    ax = axes[5]
    gamma_mean_range = np.linspace(0, 0.35, 500)
    gamma_mean_mu = GAMMA_HYPERPRIOR_MU
    gamma_mean_sigma = GAMMA_HYPERPRIOR_SIGMA
    a_trunc_gm = (0.005 - gamma_mean_mu) / gamma_mean_sigma
    b_trunc_gm = (0.3 - gamma_mean_mu) / gamma_mean_sigma
    gamma_mean_prior = truncnorm.pdf(gamma_mean_range, a_trunc_gm, b_trunc_gm,
                                      loc=gamma_mean_mu, scale=gamma_mean_sigma)
    
    samples_gamma_mean = posterior['gamma_mean_scaled'].values.flatten() / SCALING_FACTORS['gamma']
    ax.hist(samples_gamma_mean, bins=50, density=True, alpha=0.6, color='steelblue',
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(gamma_mean_range, gamma_mean_prior, 'r-', lw=2, label='Prior')
    ax.set_xlabel('Î³_mean (THz)', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('Î³_mean (Hierarchical)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 7. Î³_std (éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
    ax = axes[6]
    gamma_std_range = np.linspace(0, 0.3, 500)
    gamma_std_prior = halfnorm.pdf(gamma_std_range, scale=GAMMA_STD_PRIOR)
    
    samples_gamma_std = posterior['gamma_std_scaled'].values.flatten() / SCALING_FACTORS['gamma']
    ax.hist(samples_gamma_std, bins=50, density=True, alpha=0.6, color='steelblue',
            edgecolor='black', linewidth=0.5, label='Posterior')
    ax.plot(gamma_std_range, gamma_std_prior, 'r-', lw=2, label='Prior')
    ax.set_xlabel('Î³_std (THz)', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.set_title('Î³_std (Hierarchical)', fontsize=10, fontweight='bold')
    ax.legend(fontsize=7, loc='upper right', framealpha=0.8)
    ax.grid(alpha=0.3)
    ax.tick_params(axis='both', labelsize=8)
    
    # 8-14. gamma_1 ~ gamma_7: éšå±¤ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥Î³
    gamma_range = np.linspace(0, 0.6, 500)
    # éšå±¤ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰åˆ†å¸ƒï¼ˆWNLSãƒ™ãƒ¼ã‚¹: Î³_mean=0.074, Î³_std=0.092ï¼‰
    a_trunc_gamma = (0.005 - GAMMA_HYPERPRIOR_MU) / (GAMMA_STD_PRIOR + 1e-6)
    b_trunc_gamma = (0.5 - GAMMA_HYPERPRIOR_MU) / (GAMMA_STD_PRIOR + 1e-6)
    gamma_prior = truncnorm.pdf(gamma_range, a_trunc_gamma, b_trunc_gamma,
                                 loc=GAMMA_HYPERPRIOR_MU, scale=GAMMA_STD_PRIOR)
    
    for i in range(7):
        ax = axes[7 + i]
        var_name = f'gamma_{i+1}_scaled'
        samples_gamma = posterior[var_name].values.flatten() / SCALING_FACTORS['gamma']
        
        ax.hist(samples_gamma, bins=50, density=True, alpha=0.6, color='steelblue', 
                edgecolor='black', linewidth=0.5, label='Posterior')
        ax.plot(gamma_range, gamma_prior, 'r-', lw=2, label='Prior')
        if i < len(v6_params['gamma']):
            ax.axvline(v6_params['gamma'][i], color='orange', linestyle='--', lw=1.5, 
                      label=f'v6: {v6_params["gamma"][i]*1000:.0f}GHz')
        ax.set_xlabel(f'Î³_{i+1} (THz)', fontsize=9)
        ax.set_ylabel('Density', fontsize=9)
        ax.set_title(f'Î³_{i+1} (Pooled)', fontsize=10, fontweight='bold')
        ax.legend(fontsize=6, loc='upper right', framealpha=0.8)
        ax.grid(alpha=0.3)
        ax.tick_params(axis='both', labelsize=8)
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(14, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    
    if save_dir:
        save_path = save_dir / f'prior_posterior_comparison_{model_form}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ prior_posterior_comparison_{model_form}.png saved")
    
    plt.close()


def plot_posterior_predictive_spectra(trace, datasets, v6_params, model_form='H', save_dir=None, n_samples=500):
    """äº‹å¾Œäºˆæ¸¬é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ95% HDIåŒºé–“ + ä¸­å¤®å€¤ï¼‰"""
    print(f"\n{'='*80}")
    print(f"äº‹å¾Œäºˆæ¸¬é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ ({model_form}-form)")
    print(f"{'='*80}")
    
    posterior = trace.posterior
    
    # äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    n_chains = posterior.dims['chain']
    n_draws = posterior.dims['draw']
    total_samples = n_chains * n_draws
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰
    if total_samples > n_samples:
        sample_indices = np.random.choice(total_samples, size=n_samples, replace=False)
    else:
        sample_indices = np.arange(total_samples)
        n_samples = total_samples
    
    print(f"  äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ {n_samples} ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨")
    
    # ãƒ—ãƒ­ãƒƒãƒˆæº–å‚™
    n_datasets = len(datasets)
    ncols = 2
    nrows = (n_datasets + 1) // 2
    
    fig, axes = plt.subplots(nrows, ncols, figsize=(14, 3.5 * nrows))
    fig.suptitle(f'Posterior Predictive Transmission Spectra ({model_form}-form)', 
                 fontsize=14, y=0.995)
    axes = axes.flatten()
    
    for idx, data in enumerate(datasets):
        ax = axes[idx]
        freq = data['freq']
        trans_obs = data['trans']
        B = data['B']
        T = data['T']
        label = data['label']
        
        print(f"  è¨ˆç®—ä¸­: {label} (B={B}T, T={T}K)")
        
        # å„ã‚µãƒ³ãƒ—ãƒ«ã§é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        trans_samples = np.zeros((n_samples, len(freq)))
        
        for i, sample_idx in enumerate(sample_indices):
            chain_idx = sample_idx // n_draws
            draw_idx = sample_idx % n_draws
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£é™¤ï¼‰
            g = float(posterior['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            
            gamma_array = np.array([
                float(posterior[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            
            # é€éã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—
            trans_samples[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form
            )
        
        # çµ±è¨ˆé‡è¨ˆç®—
        trans_median = np.median(trans_samples, axis=0)
        trans_hdi = az.hdi(trans_samples, hdi_prob=0.95)
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(freq, trans_obs, 'ko', markersize=3, alpha=0.6, label='Observed')
        ax.plot(freq, trans_median, 'r-', lw=2, label='Median')
        ax.fill_between(freq, trans_hdi[:, 0], trans_hdi[:, 1], 
                        color='red', alpha=0.2, label='95% HDI')
        
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Transmission', fontsize=10)
        ax.set_title(f'{label} (B={B}T, T={T}K)', fontweight='bold')
        ax.legend(fontsize=8, loc='best')
        ax.grid(alpha=0.3)
        ax.set_xlim([freq.min(), freq.max()])
        ax.set_ylim([0, 1])
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(n_datasets, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    if save_dir:
        save_path = save_dir / f'posterior_predictive_spectra_{model_form}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ posterior_predictive_spectra_{model_form}.png saved")
    
    plt.close()
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆè¦³æ¸¬å€¤ - ä¸­å¤®å€¤ï¼‰
    print(f"\n  æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    fig, axes = plt.subplots(nrows, ncols, figsize=(14, 3.5 * nrows))
    fig.suptitle(f'Posterior Predictive Residuals ({model_form}-form)', 
                 fontsize=14, y=0.995)
    axes = axes.flatten()
    
    for idx, data in enumerate(datasets):
        ax = axes[idx]
        freq = data['freq']
        trans_obs = data['trans']
        B = data['B']
        T = data['T']
        label = data['label']
        
        # å„ã‚µãƒ³ãƒ—ãƒ«ã§é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ï¼ˆå†è¨ˆç®—ï¼‰
        trans_samples = np.zeros((n_samples, len(freq)))
        
        for i, sample_idx in enumerate(sample_indices):
            chain_idx = sample_idx // n_draws
            draw_idx = sample_idx % n_draws
            
            g = float(posterior['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            
            gamma_array = np.array([
                float(posterior[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            
            trans_samples[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form
            )
        
        trans_median = np.median(trans_samples, axis=0)
        residual = trans_obs - trans_median
        
        ax.plot(freq, residual, 'ko-', markersize=3, lw=1, alpha=0.6)
        ax.axhline(0, color='red', linestyle='--', lw=1.5)
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Residual (Obs - Pred)', fontsize=10)
        ax.set_title(f'{label} (RMSE={np.sqrt(np.mean(residual**2)):.4f})', fontweight='bold')
        ax.grid(alpha=0.3)
        ax.set_xlim([freq.min(), freq.max()])
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(n_datasets, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    if save_dir:
        save_path = save_dir / f'posterior_predictive_residuals_{model_form}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ posterior_predictive_residuals_{model_form}.png saved")
    
    plt.close()


def plot_posterior_predictive_spectra_comparison(trace_H, trace_B, datasets, save_dir=None, n_samples=500):
    """
    Hå½¢å¼ã¨Bå½¢å¼ã®äº‹å¾Œäºˆæ¸¬é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’1æšã®ã‚°ãƒ©ãƒ•ã«é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆ
    
    - Hå½¢å¼: èµ¤è‰²
    - Bå½¢å¼: é’è‰²  
    - ãƒãƒ©ãƒªãƒˆãƒ³é ˜åŸŸ: ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã§å¡—ã‚Šã¤ã¶ã—ï¼ˆé‡ã¿2.0Ã—ï¼‰
    - å…±æŒ¯å™¨é ˜åŸŸ: ç·‘è‰²ã§å¡—ã‚Šã¤ã¶ã—ï¼ˆé‡ã¿1.0Ã—ï¼‰
    - ãã‚Œä»¥å¤–: å¡—ã‚Šã¤ã¶ã—ãªã—ï¼ˆé‡ã¿0.01ï¼‰
    """
    print(f"\n{'='*80}")
    print(f"äº‹å¾Œäºˆæ¸¬é€éã‚¹ãƒšã‚¯ãƒˆãƒ«æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆ (H vs B)")
    print(f"{'='*80}")
    
    posterior_H = trace_H.posterior
    posterior_B = trace_B.posterior
    
    # äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    n_chains_H = posterior_H.dims['chain']
    n_draws_H = posterior_H.dims['draw']
    total_samples_H = n_chains_H * n_draws_H
    
    n_chains_B = posterior_B.dims['chain']
    n_draws_B = posterior_B.dims['draw']
    total_samples_B = n_chains_B * n_draws_B
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™
    if total_samples_H > n_samples:
        sample_indices_H = np.random.choice(total_samples_H, size=n_samples, replace=False)
    else:
        sample_indices_H = np.arange(total_samples_H)
        n_samples = total_samples_H
        
    if total_samples_B > n_samples:
        sample_indices_B = np.random.choice(total_samples_B, size=n_samples, replace=False)
    else:
        sample_indices_B = np.arange(total_samples_B)
    
    print(f"  äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ {n_samples} ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨")
    
    # ãƒ—ãƒ­ãƒƒãƒˆæº–å‚™
    n_datasets = len(datasets)
    n_cols = 3
    n_rows = int(np.ceil(n_datasets / n_cols))
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
    fig.suptitle('Posterior Predictive Transmission Spectra: H-form (red) vs B-form (blue)', 
                 fontsize=14, fontweight='bold', y=0.995)
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    for idx, data in enumerate(datasets):
        ax = axes[idx]
        freq = data['freq']
        trans_obs = data['trans']
        B = data['B']
        T = data['T']
        label = data['label']
        
        print(f"  è¨ˆç®—ä¸­: {label} (B={B}T, T={T}K)")
        
        # Hå½¢å¼: å„ã‚µãƒ³ãƒ—ãƒ«ã§é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        trans_samples_H = np.zeros((n_samples, len(freq)))
        
        for i, sample_idx in enumerate(sample_indices_H):
            chain_idx = sample_idx // n_draws_H
            draw_idx = sample_idx % n_draws_H
            
            g = float(posterior_H['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior_H['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior_H['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior_H['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior_H['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            
            gamma_array = np.array([
                float(posterior_H[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            
            trans_samples_H[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form='H'
            )
        
        # Bå½¢å¼: å„ã‚µãƒ³ãƒ—ãƒ«ã§é€éã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        trans_samples_B = np.zeros((n_samples, len(freq)))
        
        for i, sample_idx in enumerate(sample_indices_B):
            chain_idx = sample_idx // n_draws_B
            draw_idx = sample_idx % n_draws_B
            
            g = float(posterior_B['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior_B['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior_B['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior_B['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior_B['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            
            gamma_array = np.array([
                float(posterior_B[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            
            trans_samples_B[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form='B'
            )
        
        # çµ±è¨ˆé‡è¨ˆç®—
        trans_median_H = np.median(trans_samples_H, axis=0)
        trans_hdi_H = az.hdi(trans_samples_H, hdi_prob=0.95)
        trans_median_B = np.median(trans_samples_B, axis=0)
        trans_hdi_B = az.hdi(trans_samples_B, hdi_prob=0.95)
        
        # ãƒãƒ©ãƒªãƒˆãƒ³/å…±æŒ¯å™¨é ˜åŸŸã®æ¤œå‡ºï¼ˆHå½¢å¼åŸºæº–ï¼‰
        polariton_regions, cavity_regions = detect_peaks_and_classify(freq, trans_median_H)
        
        # é ˜åŸŸã®å¡—ã‚Šã¤ã¶ã—
        polariton_legend_added = False
        for freq_start, freq_end in polariton_regions:
            label_region = 'Polariton (2.0Ã—)' if not polariton_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.12, color='orange', label=label_region, zorder=1)
            polariton_legend_added = True
        
        cavity_legend_added = False
        for freq_start, freq_end in cavity_regions:
            label_region = 'Cavity (1.0Ã—)' if not cavity_legend_added else None
            ax.axvspan(freq_start, freq_end, alpha=0.12, color='green', label=label_region, zorder=1)
            cavity_legend_added = True
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(freq, trans_obs, 'o', color='gray', markersize=2.5, alpha=0.6, 
                label='Data', zorder=2)
        
        # Hå½¢å¼ï¼ˆèµ¤ï¼‰
        ax.plot(freq, trans_median_H, '-', color='red', linewidth=2.0, 
                label='H-form Median', zorder=4)
        ax.fill_between(freq, trans_hdi_H[:, 0], trans_hdi_H[:, 1], 
                        color='red', alpha=0.15, label='H-form 95% HDI', zorder=3)
        
        # Bå½¢å¼ï¼ˆé’ï¼‰
        ax.plot(freq, trans_median_B, '-', color='blue', linewidth=2.0, 
                label='B-form Median', zorder=4)
        ax.fill_between(freq, trans_hdi_B[:, 0], trans_hdi_B[:, 1], 
                        color='blue', alpha=0.15, label='B-form 95% HDI', zorder=3)
        
        # RMSEè¨ˆç®—
        rmse_H = np.sqrt(np.mean((trans_obs - trans_median_H)**2))
        rmse_B = np.sqrt(np.mean((trans_obs - trans_median_B)**2))
        
        ax.set_title(f"{label}\nH-RMSE: {rmse_H:.4f}, B-RMSE: {rmse_B:.4f}", 
                    fontsize=10, fontweight='bold')
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Transmittance', fontsize=10)
        ax.legend(fontsize=6, loc='best', framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--', linewidth=0.5, zorder=1)
        
        # yè»¸ç¯„å›²ã®è‡ªå‹•èª¿æ•´
        y_margin = 0.05
        y_min = min(np.min(trans_obs), np.min(trans_median_H), np.min(trans_median_B)) - y_margin
        y_max = max(np.max(trans_obs), np.max(trans_median_H), np.max(trans_median_B)) + y_margin
        ax.set_ylim(max(0, y_min), min(1.1, y_max))
        ax.set_xlim([freq.min(), freq.max()])
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(n_datasets, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    if save_dir:
        save_path = save_dir / 'posterior_predictive_spectra_HB_comparison.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ posterior_predictive_spectra_HB_comparison.png saved")
    
    plt.close()
    
    # æ®‹å·®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    print(f"\n  æ®‹å·®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
    fig.suptitle('Posterior Predictive Residuals: H-form (red) vs B-form (blue)', 
                 fontsize=14, fontweight='bold', y=0.995)
    axes = axes.flatten() if n_datasets > 1 else [axes]
    
    for idx, data in enumerate(datasets):
        ax = axes[idx]
        freq = data['freq']
        trans_obs = data['trans']
        B = data['B']
        T = data['T']
        label = data['label']
        
        # ä¸­å¤®å€¤ã‚’å†è¨ˆç®—ï¼ˆå‰ã®ãƒ«ãƒ¼ãƒ—ã§è¨ˆç®—æ¸ˆã¿ã®å ´åˆã¯ä¿å­˜ã—ã¦ãŠãã¹ãã ãŒã€ç°¡ç•¥åŒ–ã®ãŸã‚å†è¨ˆç®—ï¼‰
        trans_samples_H = np.zeros((n_samples, len(freq)))
        trans_samples_B = np.zeros((n_samples, len(freq)))
        
        for i, sample_idx in enumerate(sample_indices_H):
            chain_idx = sample_idx // n_draws_H
            draw_idx = sample_idx % n_draws_H
            g = float(posterior_H['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior_H['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior_H['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior_H['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior_H['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            gamma_array = np.array([
                float(posterior_H[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            trans_samples_H[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form='H'
            )
        
        for i, sample_idx in enumerate(sample_indices_B):
            chain_idx = sample_idx // n_draws_B
            draw_idx = sample_idx % n_draws_B
            g = float(posterior_B['g_factor_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['g']
            a = float(posterior_B['a_scale_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['a']
            B4 = float(posterior_B['B4_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B4']
            B6 = float(posterior_B['B6_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['B6']
            eps = float(posterior_B['eps_bg_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['eps']
            gamma_array = np.array([
                float(posterior_B[f'gamma_{j+1}_scaled'].values[chain_idx, draw_idx]) / SCALING_FACTORS['gamma']
                for j in range(7)
            ])
            trans_samples_B[i, :] = calculate_transmission_for_params(
                freq, B, T, g, a, B4, B6, eps, gamma_array, model_form='B'
            )
        
        trans_median_H = np.median(trans_samples_H, axis=0)
        trans_median_B = np.median(trans_samples_B, axis=0)
        
        residual_H = trans_obs - trans_median_H
        residual_B = trans_obs - trans_median_B
        
        ax.plot(freq, residual_H, 'o-', color='red', markersize=2, lw=1, alpha=0.7, label='H-form')
        ax.plot(freq, residual_B, 'o-', color='blue', markersize=2, lw=1, alpha=0.7, label='B-form')
        ax.axhline(0, color='gray', linestyle='--', lw=1.5)
        
        rmse_H = np.sqrt(np.mean(residual_H**2))
        rmse_B = np.sqrt(np.mean(residual_B**2))
        
        ax.set_xlabel('Frequency (THz)', fontsize=10)
        ax.set_ylabel('Residual (Obs - Pred)', fontsize=10)
        ax.set_title(f'{label}\nH-RMSE: {rmse_H:.4f}, B-RMSE: {rmse_B:.4f}', fontweight='bold')
        ax.legend(fontsize=8, loc='best')
        ax.grid(alpha=0.3)
        ax.set_xlim([freq.min(), freq.max()])
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(n_datasets, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    if save_dir:
        save_path = save_dir / 'posterior_predictive_residuals_HB_comparison.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ posterior_predictive_residuals_HB_comparison.png saved")
    
    plt.close()


# ============================================================================
# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆv7.1è¿½åŠ ï¼‰
# ============================================================================
DEBUG_MODE = False  # True: ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆ2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€500ã‚µãƒ³ãƒ—ãƒ«ï¼‰

# ============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================================
def main():
    global TARGET_DATA, SMC_DRAWS, SMC_CHAINS
    
    start_time = time.time()
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®š
    if DEBUG_MODE:
        print("\n" + "ğŸ”§"*40)
        print("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ON")
        print("  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: æœ€åˆã®2å€‹ã®ã¿")
        print("  - SMC Draws (H): 500")
        print("  - SMC Draws (B): 1000")
        print("  - SMC Chains: 2")
        print("ğŸ”§"*40 + "\n")
        TARGET_DATA = TARGET_DATA[:2]
        SMC_DRAWS = 500
        SMC_CHAINS = 2
    
    print(f"\n{'='*80}")
    print(f"Bayesian Analysis with Parameter Scaling and LOO-CV (v7.1: Non-centered)")
    print(f"{'='*80}")
    print(f"ã€æ–°æ©Ÿèƒ½ã€‘")
    print(f"  1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆpre_test_v6æº–æ‹ ï¼‰")
    print(f"  2. LOO-CV (Leave-One-Out Cross-Validation) ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    print(f"  3. æ¡ä»¶æ•°æ”¹å–„ã«ã‚ˆã‚‹æ•°å€¤å®‰å®šæ€§å‘ä¸Š")
    print(f"{'='*80}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã®è¡¨ç¤º
    print(f"\nğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°:")
    for key, value in SCALING_FACTORS.items():
        print(f"  {key}: {value}")
    
    # v6æœ€é©åŒ–çµæœã®èª­ã¿è¾¼ã¿ï¼ˆHå½¢å¼ã¨Bå½¢å¼ï¼‰
    print(f"\n{'='*80}")
    print("v6æœ€é©åŒ–çµæœã®èª­ã¿è¾¼ã¿")
    print(f"{'='*80}")
    v6_params_H = load_v6_optimized_params('H')
    v6_params_B = load_v6_optimized_params('B')
    
    if v6_params_H is None or v6_params_B is None:
        print("âŒ æœ€é©åŒ–çµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = pathlib.Path(__file__).parent / f"bayesian_results_scaled_loocv_{timestamp}"
    results_dir.mkdir(exist_ok=True)
    print(f"\nğŸ“ çµæœä¿å­˜: {results_dir}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    datasets = load_all_datasets(TARGET_DATA)
    if not datasets:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¨é‡ã¿
    trans_obs_concat = np.concatenate([d['trans'] for d in datasets])
    weight_concat = np.concatenate([d['weight'] for d in datasets])
    sigma_eff = 0.01 / np.sqrt(weight_concat)
    
    # ============================================================================
    # Hå½¢å¼ã§ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰ˆï¼‰
    # ============================================================================
    print(f"\n{'='*80}")
    print(f"ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆHå½¢å¼ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œï¼‰")
    print(f"{'='*80}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸäº‹å‰åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    print(f"\nğŸ“Š Hå½¢å¼äº‹å‰åˆ†å¸ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç©ºé–“ï¼‰:")
    g_scaled_mu = v6_params_H['g'] * SCALING_FACTORS['g']
    a_scaled_mu = v6_params_H['a'] * SCALING_FACTORS['a']
    B4_scaled_mu = v6_params_H['B4'] * SCALING_FACTORS['B4']
    B6_scaled_mu = v6_params_H['B6'] * SCALING_FACTORS['B6']
    eps_scaled_mu = v6_params_H['eps'] * SCALING_FACTORS['eps']
    gamma_scaled_mu = v6_params_H['gamma'] * SCALING_FACTORS['gamma']
    
    print(f"  g_scaled = {g_scaled_mu:.4f} (ç‰©ç†å€¤: {v6_params_H['g']:.4f})")
    print(f"  a_scaled = {a_scaled_mu:.4f} (ç‰©ç†å€¤: {v6_params_H['a']:.4f})")
    print(f"  B4_scaled = {B4_scaled_mu:.4f} (ç‰©ç†å€¤: {v6_params_H['B4']:.6f} K)")
    print(f"  B6_scaled = {B6_scaled_mu:.4f} (ç‰©ç†å€¤: {v6_params_H['B6']:.6f} K)")
    print(f"  eps_scaled = {eps_scaled_mu:.4f} (ç‰©ç†å€¤: {v6_params_H['eps']:.4f})")
    
    with pm.Model() as model_H:
        # ============================================================
        # äº‹å‰åˆ†å¸ƒè¨­å®šï¼ˆç‰©ç†çš„åˆ¶ç´„ãƒ™ãƒ¼ã‚¹ã€v7éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼‰
        # ============================================================
        
        # ------------------------------
        # 1. gå› å­: å¼·æƒ…å ±ï¼ˆç†è«–å€¤æº–æ‹ ï¼‰
        # ------------------------------
        g_factor_scaled_H = pm.TruncatedNormal('g_factor_scaled',
            mu=2.0 * SCALING_FACTORS['g'],      # ç†è«–å€¤
            sigma=0.05 * SCALING_FACTORS['g'],  # å¼·æƒ…å ±
            lower=1.5 * SCALING_FACTORS['g'],
            upper=2.8 * SCALING_FACTORS['g'])
        
        # ------------------------------
        # 2. a: HalfNormalï¼ˆä½å€¤å„ªå…ˆã€ä¸Šé™æ‹¡å¼µï¼‰
        # ------------------------------
        # v6ã§ a=5.0 å¼µã‚Šä»˜ã â†’ ä¸Šé™ã‚’10ã«æ‹¡å¼µ + ä½å€¤å„ªå…ˆåˆ†å¸ƒ
        a_raw_H = pm.HalfNormal('a_raw_H', sigma=2.0)  # Ïƒ=2ã§99%ãŒ0-6ã«åã¾ã‚‹
        a_scale_scaled_H = pm.Deterministic('a_scale_scaled',
            pt.clip(a_raw_H, 0.1, 10.0) * SCALING_FACTORS['a'])
        
        # ------------------------------
        # 3. Bâ‚„: LogNormalï¼ˆæ­£å€¤ä¿è¨¼ã€ä½å€¤å„ªå…ˆï¼‰
        # ------------------------------
        # Hå½¢å¼: v6ã§30mKå¼µã‚Šä»˜ã â†’ ä¸Šé™50mKã«æ‹¡å¼µ
        # LogNormal(Î¼_log, Ïƒ_log)ã§Î¼=2mK, 95%åŒºé–“â‰ˆ[0.2, 20]mK
        B4_log_mu = np.log(0.002)  # 2mKï¼ˆå¯¾æ•°å¹³å‡ï¼‰
        B4_log_sigma = 1.2         # å¯¾æ•°æ¨™æº–åå·®
        B4_raw_H = pm.LogNormal('B4_raw_H', mu=B4_log_mu, sigma=B4_log_sigma)
        B4_scaled_H = pm.Deterministic('B4_scaled',
            pt.clip(B4_raw_H, 0.00001, 0.05) * SCALING_FACTORS['B4'])  # [0.01mK, 50mK]
        
        # ------------------------------
        # 4. Bâ‚†: Normalï¼ˆã‚¼ãƒ­ä¸­å¿ƒã€å¯¾ç§°ï¼‰
        # ------------------------------
        # v6çµæœ: H=-1.0mK, B=-1.0mK â†’ ã»ã¼ä¸‹é™
        # Normal(0, 0.5mK)ã§95%åŒºé–“â‰ˆ[-1mK, +1mK]ã€ç¯„å›²æ‹¡å¼µ
        B6_raw_H = pm.Normal('B6_raw_H', mu=0, sigma=0.0005)
        B6_scaled_H = pm.Deterministic('B6_scaled',
            pt.clip(B6_raw_H, -0.002, 0.002) * SCALING_FACTORS['B6'])  # [-2mK, +2mK]
        
        # ------------------------------
        # 5. Îµ_bg: ä¸­æƒ…å ±ï¼ˆæ–‡çŒ®ç¯„å›²+v6å‚è€ƒï¼‰
        # ------------------------------
        # v6: H=14.0, B=14.1 â†’ ä¸¡æ–¹ã¨ã‚‚å¦¥å½“ãªç¯„å›²
        eps_v6_avg = (v6_params_H['eps'] + v6_params_B['eps']) / 2  # v6å¹³å‡å€¤ã‚’å‚è€ƒ
        eps_bg_scaled_H = pm.TruncatedNormal('eps_bg_scaled',
            mu=eps_v6_avg * SCALING_FACTORS['eps'],
            sigma=0.3 * SCALING_FACTORS['eps'],  # 0.5 â†’ 0.3ï¼ˆæƒ…å ±å¼·åŒ–ï¼‰
            lower=13.0 * SCALING_FACTORS['eps'],
            upper=16.0 * SCALING_FACTORS['eps'])
        
        # ------------------------------
        # 6. Î³: Non-centeredéšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆè­˜åˆ¥ä¸èƒ½æ€§è§£æ¶ˆ + åæŸæ€§æ”¹å–„ï¼‰
        # ------------------------------
        # v7.1: Non-centered Parameterizationã§ã€Œæ¼æ–—ã€å•é¡Œã‚’è§£æ¶ˆ
        # Centeredç‰ˆã§ã¯Î³_meanãŒå¤‰åŒ–ã™ã‚‹ã¨å…¨Î³_iã®æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒé€£å‹•å¤‰åŒ–
        # Non-centeredç‰ˆã§ã¯z_iã¨Î³_meanãŒç‹¬ç«‹ã§ã€HMC/SMCãŒåŠ¹ç‡çš„ã«æ¢ç´¢å¯èƒ½
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogç©ºé–“ã§å®šç¾© â†’ æ­£å€¤ä¿è¨¼ï¼‰
        log_gamma_mu_H = pm.Normal('log_gamma_mu',
            mu=np.log(GAMMA_HYPERPRIOR_MU),  # log(0.074) â‰ˆ -2.6
            sigma=0.3)  # logç©ºé–“ã§ç·©ã‚ã®äº‹å‰åˆ†å¸ƒ
        
        log_gamma_sd_H = pm.HalfNormal('log_gamma_sd', sigma=0.3)
        
        # â˜…â˜…â˜… Non-centeredå¤‰æ›ã®æ ¸å¿ƒ â˜…â˜…â˜…
        # æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆä¸Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ç‹¬ç«‹ï¼‰
        gamma_raw_H = pm.Normal('gamma_raw', mu=0, sigma=1, shape=7)
        
        # æ±ºå®šè«–çš„å¤‰æ›ã§ç‰©ç†å€¤ã«å¤‰æ›
        # log(Î³_i) = log(Î³_mu) + log(Î³_sd) * z_i
        # Î³_i = exp(log(Î³_mu) + log(Î³_sd) * z_i)
        gamma_vec_unscaled_H = pm.Deterministic('gamma_vec',
            pt.exp(log_gamma_mu_H + log_gamma_sd_H * gamma_raw_H))
        
        # åˆ‡ã‚Šæ¨ã¦å‡¦ç†ï¼ˆç‰©ç†çš„ã«æ„å‘³ã®ã‚ã‚‹ç¯„å›²ï¼‰+ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        gamma_vec_scaled_H = pm.Deterministic('gamma_vec_scaled',
            pt.clip(gamma_vec_unscaled_H, 0.005, 0.5) * SCALING_FACTORS['gamma'])
        
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚å€‹åˆ¥gamma_i_scaledã‚‚å®šç¾©
        for i in range(7):
            pm.Deterministic(f'gamma_{i+1}_scaled', gamma_vec_scaled_H[i])
        
        # éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰©ç†å€¤ç©ºé–“ã§è¨˜éŒ²ï¼ˆè¨ºæ–­ç”¨ï¼‰
        gamma_mean_scaled_H = pm.Deterministic('gamma_mean_scaled',
            pt.exp(log_gamma_mu_H) * SCALING_FACTORS['gamma'])
        gamma_std_scaled_H = pm.Deterministic('gamma_std_scaled',
            log_gamma_sd_H * SCALING_FACTORS['gamma'])
        
        # ------------------------------
        # 7. å°¤åº¦: StudentTï¼ˆå¤–ã‚Œå€¤é ‘å¥ï¼‰
        # ------------------------------
        model_op_H = ScaledInformedPriorModelOp(datasets, 'H')
        trans_pred_H = model_op_H(a_scale_scaled_H, gamma_vec_scaled_H,
                                   g_factor_scaled_H, B4_scaled_H,
                                   B6_scaled_H, eps_bg_scaled_H)
        
        likelihood_H = pm.StudentT('likelihood',
            nu=NU_STUDENTT,
            mu=trans_pred_H,
            sigma=sigma_eff,
            observed=trans_obs_concat)
        
        # ------------------------------
        # 8. SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        # ------------------------------
        print(f"\nğŸ”¬ SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ï¼ˆHå½¢å¼ï¼‰")
        print(f"   Sampler: SMC (Sequential Monte Carlo)")
        print(f"   Draws: {SMC_DRAWS}, Chains: {SMC_CHAINS}, Cores: {SMC_CHAINS if SMC_PARALLEL else 1}")
        print(f"   Hierarchical Î³: ON, Likelihood: StudentT(Î½={NU_STUDENTT})")
        
        trace_H = pm.sample_smc(
            draws=SMC_DRAWS,
            chains=SMC_CHAINS,
            cores=SMC_CHAINS if SMC_PARALLEL else 1,
            return_inferencedata=True,
            progressbar=True,
            random_seed=RANDOM_SEED,
        )
        
        # SMCã¯è‡ªå‹•çš„ã«log_likelihoodã‚’ä¿å­˜ã—ãªã„ãŸã‚ã€æ˜ç¤ºçš„ã«è¨ˆç®—
        print(f"\nğŸ“Š log_likelihoodè¨ˆç®—ä¸­ï¼ˆHå½¢å¼ï¼‰...")
        pm.compute_log_likelihood(trace_H, model=model_H)
    
    print(f"\nâœ… Hå½¢å¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†ï¼ˆlog_likelihoodä¿å­˜æ¸ˆã¿ï¼‰")
    
    # ============================================================================
    # Bå½¢å¼ã§ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰ˆï¼‰
    # ============================================================================
    print(f"\n{'='*80}")
    print(f"ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆBå½¢å¼ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œï¼‰")
    print(f"{'='*80}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸäº‹å‰åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    print(f"\nğŸ“Š Bå½¢å¼äº‹å‰åˆ†å¸ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç©ºé–“ï¼‰:")
    g_scaled_mu_B = v6_params_B['g'] * SCALING_FACTORS['g']
    a_scaled_mu_B = v6_params_B['a'] * SCALING_FACTORS['a']
    B4_scaled_mu_B = v6_params_B['B4'] * SCALING_FACTORS['B4']
    B6_scaled_mu_B = v6_params_B['B6'] * SCALING_FACTORS['B6']
    eps_scaled_mu_B = v6_params_B['eps'] * SCALING_FACTORS['eps']
    gamma_scaled_mu_B = v6_params_B['gamma'] * SCALING_FACTORS['gamma']
    
    print(f"  g_scaled = {g_scaled_mu_B:.4f} (ç‰©ç†å€¤: {v6_params_B['g']:.4f})")
    print(f"  a_scaled = {a_scaled_mu_B:.4f} (ç‰©ç†å€¤: {v6_params_B['a']:.4f})")
    print(f"  B4_scaled = {B4_scaled_mu_B:.4f} (ç‰©ç†å€¤: {v6_params_B['B4']:.6f} K)")
    print(f"  B6_scaled = {B6_scaled_mu_B:.4f} (ç‰©ç†å€¤: {v6_params_B['B6']:.6f} K)")
    print(f"  eps_scaled = {eps_scaled_mu_B:.4f} (ç‰©ç†å€¤: {v6_params_B['eps']:.4f})")
    
    with pm.Model() as model_B:
        # ============================================================
        # äº‹å‰åˆ†å¸ƒè¨­å®šï¼ˆç‰©ç†çš„åˆ¶ç´„ãƒ™ãƒ¼ã‚¹ã€v7éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼‰- Bå½¢å¼
        # ============================================================
        
        # ------------------------------
        # 1. gå› å­: å¼·æƒ…å ±ï¼ˆç†è«–å€¤æº–æ‹ ï¼‰
        # ------------------------------
        g_factor_scaled_B = pm.TruncatedNormal('g_factor_scaled',
            mu=2.0 * SCALING_FACTORS['g'],      # ç†è«–å€¤
            sigma=0.05 * SCALING_FACTORS['g'],  # å¼·æƒ…å ±
            lower=1.5 * SCALING_FACTORS['g'],
            upper=2.8 * SCALING_FACTORS['g'])
        
        # ------------------------------
        # 2. a: HalfNormalï¼ˆä½å€¤å„ªå…ˆã€ä¸Šé™æ‹¡å¼µï¼‰
        # ------------------------------
        a_raw_B = pm.HalfNormal('a_raw_B', sigma=2.0)
        a_scale_scaled_B = pm.Deterministic('a_scale_scaled',
            pt.clip(a_raw_B, 0.1, 10.0) * SCALING_FACTORS['a'])
        
        # ------------------------------
        # 3. Bâ‚„: LogNormalï¼ˆæ­£å€¤ä¿è¨¼ã€ä½å€¤å„ªå…ˆï¼‰
        # ------------------------------
        B4_log_mu_B = np.log(0.002)
        B4_log_sigma_B = 1.2
        B4_raw_B = pm.LogNormal('B4_raw_B', mu=B4_log_mu_B, sigma=B4_log_sigma_B)
        B4_scaled_B = pm.Deterministic('B4_scaled',
            pt.clip(B4_raw_B, 0.00001, 0.05) * SCALING_FACTORS['B4'])
        
        # ------------------------------
        # 4. Bâ‚†: Normalï¼ˆã‚¼ãƒ­ä¸­å¿ƒã€å¯¾ç§°ï¼‰
        # ------------------------------
        B6_raw_B = pm.Normal('B6_raw_B', mu=0, sigma=0.0005)
        B6_scaled_B = pm.Deterministic('B6_scaled',
            pt.clip(B6_raw_B, -0.002, 0.002) * SCALING_FACTORS['B6'])
        
        # ------------------------------
        # 5. Îµ_bg: ä¸­æƒ…å ±ï¼ˆæ–‡çŒ®ç¯„å›²+v6å‚è€ƒï¼‰
        # ------------------------------
        eps_v6_avg_B = (v6_params_H['eps'] + v6_params_B['eps']) / 2
        eps_bg_scaled_B = pm.TruncatedNormal('eps_bg_scaled',
            mu=eps_v6_avg_B * SCALING_FACTORS['eps'],
            sigma=0.3 * SCALING_FACTORS['eps'],
            lower=13.0 * SCALING_FACTORS['eps'],
            upper=16.0 * SCALING_FACTORS['eps'])
        
        # ------------------------------
        # 6. Î³: Non-centeredéšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆè­˜åˆ¥ä¸èƒ½æ€§è§£æ¶ˆ + åæŸæ€§æ”¹å–„ï¼‰
        # ------------------------------
        # v7.1: Non-centered Parameterizationï¼ˆBå½¢å¼ã‚‚åŒæ§˜ã«é©ç”¨ï¼‰
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogç©ºé–“ã§å®šç¾© â†’ æ­£å€¤ä¿è¨¼ï¼‰
        # Bå½¢å¼: äº‹å‰åˆ†å¸ƒå¼·åŒ–ï¼ˆè­˜åˆ¥ä¸èƒ½æ€§å¯¾ç­–ï¼‰
        log_gamma_mu_B = pm.Normal('log_gamma_mu',
            mu=np.log(GAMMA_HYPERPRIOR_MU),
            sigma=0.3)  # 0.5â†’0.3 ã‚ˆã‚Šæƒ…å ±çš„ã«
        
        log_gamma_sd_B = pm.HalfNormal('log_gamma_sd', sigma=0.3)  # 0.5â†’0.3 ã‚ˆã‚Šåˆ¶ç´„
        
        # Non-centeredå¤‰æ›
        gamma_raw_B = pm.Normal('gamma_raw', mu=0, sigma=1, shape=7)
        
        gamma_vec_unscaled_B = pm.Deterministic('gamma_vec',
            pt.exp(log_gamma_mu_B + log_gamma_sd_B * gamma_raw_B))
        
        gamma_vec_scaled_B = pm.Deterministic('gamma_vec_scaled',
            pt.clip(gamma_vec_unscaled_B, 0.005, 0.5) * SCALING_FACTORS['gamma'])
        
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚å€‹åˆ¥gamma_i_scaledã‚‚å®šç¾©
        for i in range(7):
            pm.Deterministic(f'gamma_{i+1}_scaled', gamma_vec_scaled_B[i])
        
        # éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰©ç†å€¤ç©ºé–“ã§è¨˜éŒ²ï¼ˆè¨ºæ–­ç”¨ï¼‰
        gamma_mean_scaled_B = pm.Deterministic('gamma_mean_scaled',
            pt.exp(log_gamma_mu_B) * SCALING_FACTORS['gamma'])
        gamma_std_scaled_B = pm.Deterministic('gamma_std_scaled',
            log_gamma_sd_B * SCALING_FACTORS['gamma'])
        
        # ------------------------------
        # 7. å°¤åº¦: StudentTï¼ˆå¤–ã‚Œå€¤é ‘å¥ï¼‰
        # ------------------------------
        model_op_B = ScaledInformedPriorModelOp(datasets, 'B')
        trans_pred_B = model_op_B(a_scale_scaled_B, gamma_vec_scaled_B,
                                   g_factor_scaled_B, B4_scaled_B,
                                   B6_scaled_B, eps_bg_scaled_B)
        
        likelihood_B = pm.StudentT('likelihood',
            nu=NU_STUDENTT,
            mu=trans_pred_B,
            sigma=sigma_eff,
            observed=trans_obs_concat)
        
        # ------------------------------
        # 8. SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆBå½¢å¼å°‚ç”¨å¼·åŒ–è¨­å®šï¼‰
        # ------------------------------
        print(f"\nğŸ”¬ SMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ï¼ˆBå½¢å¼ - å¼·åŒ–è¨­å®šï¼‰")
        print(f"   Sampler: SMC (Sequential Monte Carlo)")
        print(f"   Draws: {SMC_DRAWS}, Chains: {SMC_CHAINS}, Cores: {SMC_CHAINS if SMC_PARALLEL else 1}")
        print(f"   âš¡ Bå½¢å¼å¼·åŒ–: ã‚µãƒ³ãƒ—ãƒ«æ•°2å€ã€ãƒã‚§ãƒ¼ãƒ³æ•°2å€ã€äº‹å‰åˆ†å¸ƒå¼·åŒ–")
        print(f"   Hierarchical Î³: ON, Likelihood: StudentT(Î½={NU_STUDENTT})")
        
        trace_B = pm.sample_smc(
            draws=SMC_DRAWS,
            chains=SMC_CHAINS,
            cores=SMC_CHAINS if SMC_PARALLEL else 1,
            return_inferencedata=True,
            progressbar=True,
            random_seed=RANDOM_SEED
        )
        
        # SMCã¯è‡ªå‹•çš„ã«log_likelihoodã‚’ä¿å­˜ã—ãªã„ãŸã‚ã€æ˜ç¤ºçš„ã«è¨ˆç®—
        print(f"\nğŸ“Š log_likelihoodè¨ˆç®—ä¸­ï¼ˆBå½¢å¼ï¼‰...")
        pm.compute_log_likelihood(trace_B, model=model_B)
    
    print(f"\nâœ… Bå½¢å¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†ï¼ˆlog_likelihoodã®ä¿å­˜æ¸ˆã¿ï¼‰")
    
    # ============================================================================
    # LOO-CVè©•ä¾¡
    # ============================================================================
    loo_H = compute_loo_cv(trace_H, 'H-form')
    loo_B = compute_loo_cv(trace_B, 'B-form')
    
    comparison_result = None
    if loo_H is not None and loo_B is not None:
        comparison_result = compare_models_loo(loo_H, loo_B)
    
    # ============================================================================
    # ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—
    # ============================================================================
    bf_result = compute_bayes_factor_smc(trace_H, trace_B)
    
    # ============================================================================
    # å¯è¦–åŒ–ï¼ˆä¿®å£«è«–æ–‡ç”¨ï¼‰
    # ============================================================================
    print(f"\n{'='*80}")
    print("ğŸ“Š ä¿®å£«è«–æ–‡ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ")
    print(f"{'='*80}")
    
    # Hå½¢å¼
    plot_prior_distributions(v6_params_H, 'H', results_dir)
    plot_posterior_distributions(trace_H, 'H', results_dir)
    plot_prior_posterior_comparison(trace_H, v6_params_H, 'H', results_dir)
    plot_posterior_predictive_spectra(trace_H, datasets, v6_params_H, 'H', results_dir)
    
    # Bå½¢å¼
    plot_prior_distributions(v6_params_B, 'B', results_dir)
    plot_posterior_distributions(trace_B, 'B', results_dir)
    plot_prior_posterior_comparison(trace_B, v6_params_B, 'B', results_dir)
    plot_posterior_predictive_spectra(trace_B, datasets, v6_params_B, 'B', results_dir)
    
    # Hå½¢å¼ vs Bå½¢å¼ æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    plot_posterior_predictive_spectra_comparison(trace_H, trace_B, datasets, results_dir)
    
    print(f"\nâœ… å…¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†")
    
    # ============================================================================
    # çµæœä¿å­˜
    # ============================================================================
    print(f"\n{'='*80}")
    print("çµæœä¿å­˜")
    print(f"{'='*80}")
    
    # ãƒˆãƒ¬ãƒ¼ã‚¹ä¿å­˜ï¼ˆSMCã®å ´åˆã€sample_statsã«mixed typeãŒã‚ã‚‹ãŸã‚pickleå½¢å¼ã‚‚ç”¨æ„ï¼‰
    try:
        trace_H.to_netcdf(str(results_dir / 'trace_H.nc'))
        print("  âœ“ trace_H.nc")
    except ValueError as e:
        print(f"  âš ï¸ netCDFä¿å­˜å¤±æ•— (SMC betaæ··åœ¨å‹): {e}")
        # pickleå½¢å¼ã§ä¿å­˜
        import pickle
        with open(results_dir / 'trace_H.pkl', 'wb') as f:
            pickle.dump(trace_H, f)
        print("  âœ“ trace_H.pkl (pickleå½¢å¼)")
    
    try:
        trace_B.to_netcdf(str(results_dir / 'trace_B.nc'))
        print("  âœ“ trace_B.nc")
    except ValueError as e:
        print(f"  âš ï¸ netCDFä¿å­˜å¤±æ•— (SMC betaæ··åœ¨å‹): {e}")
        import pickle
        with open(results_dir / 'trace_B.pkl', 'wb') as f:
            pickle.dump(trace_B, f)
        print("  âœ“ trace_B.pkl (pickleå½¢å¼)")
    
    # ã‚µãƒãƒªãƒ¼ä¿å­˜
    summary_H = az.summary(trace_H)
    summary_H.to_csv(results_dir / 'summary_H.csv')
    print("  âœ“ summary_H.csv")
    
    summary_B = az.summary(trace_B)
    summary_B.to_csv(results_dir / 'summary_B.csv')
    print("  âœ“ summary_B.csv")
    
    # ç‰©ç†å€¤ã¸ã®å¤‰æ›ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£é™¤ï¼‰- v7éšå±¤ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
    posterior_H = trace_H.posterior
    params_H = {
        'g': float(posterior_H['g_factor_scaled'].mean()) / SCALING_FACTORS['g'],
        'a': float(posterior_H['a_scale_scaled'].mean()) / SCALING_FACTORS['a'],
        'B4': float(posterior_H['B4_scaled'].mean()) / SCALING_FACTORS['B4'],
        'B6': float(posterior_H['B6_scaled'].mean()) / SCALING_FACTORS['B6'],
        'eps': float(posterior_H['eps_bg_scaled'].mean()) / SCALING_FACTORS['eps'],
        'gamma': np.array([float(posterior_H[f'gamma_{i+1}_scaled'].mean()) / SCALING_FACTORS['gamma'] for i in range(7)]),
        # éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆv7æ–°è¦ï¼‰
        'gamma_mean': float(posterior_H['gamma_mean_scaled'].mean()) / SCALING_FACTORS['gamma'],
        'gamma_std': float(posterior_H['gamma_std_scaled'].mean()) / SCALING_FACTORS['gamma'],
    }
    
    posterior_B = trace_B.posterior
    params_B = {
        'g': float(posterior_B['g_factor_scaled'].mean()) / SCALING_FACTORS['g'],
        'a': float(posterior_B['a_scale_scaled'].mean()) / SCALING_FACTORS['a'],
        'B4': float(posterior_B['B4_scaled'].mean()) / SCALING_FACTORS['B4'],
        'B6': float(posterior_B['B6_scaled'].mean()) / SCALING_FACTORS['B6'],
        'eps': float(posterior_B['eps_bg_scaled'].mean()) / SCALING_FACTORS['eps'],
        'gamma': np.array([float(posterior_B[f'gamma_{i+1}_scaled'].mean()) / SCALING_FACTORS['gamma'] for i in range(7)]),
        # éšå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆv7æ–°è¦ï¼‰
        'gamma_mean': float(posterior_B['gamma_mean_scaled'].mean()) / SCALING_FACTORS['gamma'],
        'gamma_std': float(posterior_B['gamma_std_scaled'].mean()) / SCALING_FACTORS['gamma'],
    }
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
    params_H_df = pd.DataFrame([{k: v if not isinstance(v, np.ndarray) else v.tolist() for k, v in params_H.items()}])
    params_H_df.to_csv(results_dir / 'parameters_H.csv', index=False)
    print("  âœ“ parameters_H.csv")
    
    params_B_df = pd.DataFrame([{k: v if not isinstance(v, np.ndarray) else v.tolist() for k, v in params_B.items()}])
    params_B_df.to_csv(results_dir / 'parameters_B.csv', index=False)
    print("  âœ“ parameters_B.csv")
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœä¿å­˜ï¼ˆv7: SMCå¯¾å¿œ + BFè¿½åŠ ï¼‰
    if loo_H is not None and loo_B is not None:
        eval_results = {
            'H_form': loo_H,  # compute_model_evaluationã®çµæœ
            'B_form': loo_B,
            'comparison_waic': comparison_result if comparison_result else {},
            'comparison_bayes_factor': bf_result if bf_result else {},
            'timestamp': timestamp,
            'sampler': SAMPLER_TYPE,
            'likelihood': LIKELIHOOD_TYPE,
            'hierarchical_gamma': USE_HIERARCHICAL_GAMMA
        }
        
        with open(results_dir / 'model_evaluation.json', 'w') as f:
            json.dump(eval_results, f, indent=2, default=str)
        print("  âœ“ model_evaluation.json")
    
    # å®Œäº†
    total_time = time.time() - start_time
    print(f"\n{'='*80}")
    print("ğŸ‰ å…¨å‡¦ç†å®Œäº†")
    print(f"{'='*80}")
    print(f"  å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)")
    print(f"  çµæœ: {results_dir}")
    print(f"  ãƒ¢ãƒ‡ãƒ«å½¢å¼: H-form & B-form (ä¸¡æ–¹)")
    print(f"  ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ (H): {SAMPLER_TYPE} (Draws={SMC_DRAWS}, Chains={SMC_CHAINS})")
    print(f"  ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ (B): {SAMPLER_TYPE} (Draws={SMC_DRAWS}, Chains={SMC_CHAINS}) [å¼·åŒ–]")
    print(f"  å°¤åº¦: {LIKELIHOOD_TYPE} (Î½={NU_STUDENTT})")
    print(f"  éšå±¤Î³ãƒ¢ãƒ‡ãƒ«: {'ON' if USE_HIERARCHICAL_GAMMA else 'OFF'}")
    if comparison_result is not None:
        print(f"  æ¨å¥¨ãƒ¢ãƒ‡ãƒ« (WAIC): {comparison_result.get('winner', 'N/A')}")
    if bf_result is not None:
        print(f"  æ¨å¥¨ãƒ¢ãƒ‡ãƒ« (BayesFactor): {bf_result.get('winner', 'N/A')} (logBF={bf_result.get('log_BF', 0):.2f})")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()
